{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Magic functinos\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### imports\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mliv.dgps import get_data, get_tau_fn, fn_dict\n",
    "from mliv.neuralnet.deepiv_fit import deep_iv_fit\n",
    "from mliv.neuralnet.utilities import log_metrics, plot_results\n",
    "from mliv.neuralnet.rbflayer import gaussian, inverse_multiquadric\n",
    "from mliv.neuralnet import AGMM, KernelLayerMMDGMM, CentroidMMDGMM, KernelLossAGMM, MMDGMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "n_z = 3\n",
    "n_t = 3\n",
    "iv_strength = .6\n",
    "fname = 'abs'\n",
    "dgp_num = 5\n",
    "Z, T, Y, true_fn = get_data(n, n_z, iv_strength, get_tau_fn(fn_dict[fname]), dgp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "x_grid = np.linspace(np.quantile(T[:, ind], .01), np.quantile(T[:, ind], .99), 100)\n",
    "T_test = np.zeros((100, T.shape[1])) + np.median(T, axis=0, keepdims=True)\n",
    "T_test[:, ind] = x_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(Z[:, 0], Y)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(T[:, 0], Y)\n",
    "plt.plot(T[np.argsort(T[:, 0]), 0], true_fn(T[np.argsort(T[:, 0])]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepIV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deep_iv = deep_iv_fit(Z[:,[0]], T[:,[0]], Y, x=T[:,1:],\n",
    "                      epochs=100, hidden=[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = deep_iv.predict([T_test[:, 1:], T_test[:, [0]]])\n",
    "plt.scatter(true_fn(T_test).flatten(), y_pred)\n",
    "plt.title(1 - np.mean((true_fn(T_test).flatten()-y_pred.flatten())**2) / np.var(true_fn(T_test).flatten()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGMM Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learner Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.1 # dropout prob of dropout layers throughout notebook\n",
    "n_hidden = 100 # width of hidden layers throughout notebook\n",
    "\n",
    "learner = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_t, n_hidden), nn.LeakyReLU(),\n",
    "                        #nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
    "                        nn.Dropout(p=p), nn.Linear(n_hidden, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversary Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For any method that use a projection of z into features g(z)\n",
    "g_features = 100\n",
    "adversary_g = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_z, n_hidden), nn.LeakyReLU(),\n",
    "                            nn.Dropout(p=p), nn.Linear(n_hidden, g_features), nn.ReLU())\n",
    "# The kernel function\n",
    "kernel_fn = gaussian\n",
    "# kernel_fn = inverse_multiquadric\n",
    "\n",
    "# For any method that uses an unstructured adversary test function f(z)\n",
    "adversary_fn = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_z, n_hidden), nn.LeakyReLU(),\n",
    "                             #nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
    "                             nn.Dropout(p=p), nn.Linear(n_hidden, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_lr = 1e-4\n",
    "adversary_lr = 1e-4\n",
    "learner_l2 = 1e-3\n",
    "adversary_l2 = 1e-4\n",
    "adversary_norm_reg = 1e-3\n",
    "n_epochs = 300\n",
    "bs = 100\n",
    "sigma = 2.0/g_features\n",
    "n_centers = 100\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Val Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "Z_train, Z_val, T_train, T_val, Y_train, Y_val = train_test_split(Z, T, Y, test_size=.1, shuffle=True)\n",
    "Z_train, T_train, Y_train = map(lambda x: torch.Tensor(x), (Z_train, T_train, Y_train))\n",
    "Z_val, T_val, Y_val = map(lambda x: torch.Tensor(x).to(device), (Z_val, T_val, Y_val))\n",
    "G_train = true_fn(T_train)\n",
    "G_val = true_fn(T_val)\n",
    "G_train, G_val = map(lambda x: x.to(device), (G_train,G_val))\n",
    "T_test_tens = torch.Tensor(T_test).to(device)\n",
    "G_test_tens = true_fn(T_test_tens).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_2$-Regularized AGMM with Neural Net Test Function\n",
    "\n",
    "We solve the problem:\n",
    "\\begin{equation}\n",
    "\\min_{\\theta} \\max_{w} \\frac{1}{n} \\sum_i (y_i - h_{\\theta}(x_i)) f_w(z_i) - f_w(z_i)^2\n",
    "\\end{equation}\n",
    "where $h_{\\theta}$ and $f_w$ are two neural nets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(learner, adversary, epoch, writer):\n",
    "    writer.add_histogram('learner', learner[-1].weight, epoch)\n",
    "    writer.add_histogram('adversary', adversary[-1].weight, epoch)\n",
    "    log_metrics(Z_val, T_val, Y_val, Z_val, T_val, Y_val, T_test_tens,\n",
    "                learner, adversary, epoch, writer, true_of_T=G_val)\n",
    "\n",
    "np.random.seed(12356)\n",
    "agmm = AGMM(learner, adversary_fn).fit(Z_train, T_train, Y_train, learner_lr=learner_lr, adversary_lr=adversary_lr,\n",
    "                                       learner_l2=learner_l2, adversary_l2=adversary_l2,\n",
    "                                       n_epochs=n_epochs, bs=bs, logger=logger,\n",
    "                                       model_dir='agmm_model', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(agmm, T_test_tens, G_test_tens, ind=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_2$-regularized MMD-GMM with Fixed Kernel Points and No Kernel Learning\n",
    "\n",
    "We choose a fixed set of centers and approximate the kernel function with the kernel distance to these centers.\n",
    "\n",
    "Here we learn only the $\\beta$ and fix $c_i, \\sigma_i$, i.e.:\n",
    "\\begin{align}\n",
    "f(z) = \\sum_{i=1}^s \\beta_i K_{\\sigma_i}(c_i, z_i)\n",
    "\\end{align}\n",
    "and both $c_i$ is a grid and $\\sigma$ is a constant. We are also approximating the RKHS norm with:\n",
    "\\begin{align}\n",
    "\\|f\\|_{K}^2 = \\sum_{i, j \\in [s]} \\beta_i \\beta_j \\frac{K_{\\sigma_i}(c_i, c_j) + K_{\\sigma_j}(c_j, c_i)}{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(learner, adversary, epoch, writer):\n",
    "    writer.add_histogram('learner', learner[-1].weight, epoch)\n",
    "    writer.add_histogram('adversary', adversary.beta.weight, epoch)\n",
    "    log_metrics(Z_val, T_val, Y_val, Z_val, T_val, Y_val, T_test_tens,\n",
    "                learner, adversary, epoch, writer, true_of_T=G_val)\n",
    "\n",
    "# Fixed centers c_1, ..., c_s and kernel precision sigma_1, ..., sigma_s\n",
    "centers = np.tile(np.linspace(-4, 4, n_centers).reshape(-1, 1), (1, n_z))\n",
    "sigmas = np.ones((n_centers,)) * 2/n_z\n",
    "\n",
    "mmdgmm_fixed = KernelLayerMMDGMM(learner, lambda x: x, n_z, n_centers, kernel_fn,\n",
    "                      centers=centers, sigmas=sigmas, trainable=False)\n",
    "mmdgmm_fixed.fit(Z_train, T_train, Y_train, learner_l2=learner_l2, adversary_l2=adversary_l2,\n",
    "                 adversary_norm_reg=adversary_norm_reg,\n",
    "                 learner_lr=learner_lr, adversary_lr=adversary_lr, n_epochs=n_epochs, bs=bs, logger=logger,\n",
    "                 model_dir='mmd_fixed_model', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(mmdgmm_fixed, T_test_tens, G_test_tens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_2$-regularized MMD-GMM with Kernel Learning and Approximation via Kernel Layer\n",
    "\n",
    "\n",
    "Here we learn both the G function and the centers and sigmas, i.e.:\n",
    "\\begin{align}\n",
    "f(z) = \\sum_{i=1}^s \\beta_i K_{\\sigma_i}(c_i, g(z_i))\n",
    "\\end{align}\n",
    "and both $c_i$ and $\\sigma_i$ are also trained. $c_i$ are $n_{\\text{features}}$-dimensional vectors, initialized to some inital set of grid values. Moreover, $g(z)$ is a fully connected network with RELU gates. We are also approximating the RKHS norm with:\n",
    "\\begin{align}\n",
    "\\|f\\|_{K}^2 = \\sum_{i, j \\in [s]} \\beta_i \\beta_j \\frac{K_{\\sigma_i}(c_i, c_j) + K_{\\sigma_j}(c_j, c_i)}{2}\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(learner, adversary, epoch, writer):\n",
    "    writer.add_histogram('learner', learner[-1].weight, epoch)\n",
    "    writer.add_histogram('adversary', adversary.beta.weight, epoch)\n",
    "    log_metrics(Z_val, T_val, Y_val, Z_val, T_val, Y_val, T_test_tens,\n",
    "                learner, adversary, epoch, writer, true_of_T=G_val)\n",
    "\n",
    "# Trainable centers c_1, ..., c_s and precisions sigma_1, ..., sigma_s initialization\n",
    "#centers = np.tile(np.linspace(-4, 4, n_centers).reshape(-1, 1), (1, g_features))\n",
    "centers = np.random.uniform(-4, 4, size=(n_centers, g_features))\n",
    "sigmas = np.ones((n_centers,)) * sigma\n",
    "\n",
    "klayermmdgmm = KernelLayerMMDGMM(learner, adversary_g, g_features,\n",
    "                                 n_centers, kernel_fn, centers=centers, sigmas=sigmas)\n",
    "klayermmdgmm.fit(Z_train, T_train, Y_train, learner_l2=learner_l2, adversary_l2=adversary_l2,\n",
    "                 adversary_norm_reg=adversary_norm_reg,\n",
    "                 learner_lr=learner_lr, adversary_lr = adversary_lr,  n_epochs=n_epochs, bs=bs, logger=logger,\n",
    "                 model_dir='klayer_model', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(klayermmdgmm, T_test_tens, G_test_tens, ind=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_2$-regularized MMD-GMM with Kernel Learning and Approximation via KMeans Centroids\n",
    "\n",
    "Here we do exactly what is described in the paper for low rank approximation. We choose centers of a kmeans clusters in the $z$-space, $c_1, \\ldots, c_m$ and then we test function of the form:\n",
    "\\begin{equation}\n",
    "f(z) = \\sum_{i=1}^m \\beta_i K_{\\sigma_i}(g_w(c_i), g_w(z))\n",
    "\\end{equation}\n",
    "and penalizes the approximate RKHS norm:\n",
    "\\begin{align}\n",
    "\\|f\\|_{K}^2 = \\sum_{i, j \\in [s]} \\beta_i \\beta_j \\frac{K_{\\sigma_i}(g_w(z_i), g_w(z_j)) + K_{\\sigma_j}(g_w(z_j), g_w(z_i))}{2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(learner, adversary, epoch, writer):\n",
    "    writer.add_histogram('learner', learner[-1].weight, epoch)\n",
    "    writer.add_histogram('adversary', adversary.beta.weight, epoch)\n",
    "    log_metrics(Z_val, T_val, Y_val, Z_val, T_val, Y_val, T_test_tens,\n",
    "                learner, adversary, epoch, writer, true_of_T=G_val)\n",
    "\n",
    "# Kmeans based centers z_1, ..., z_s\n",
    "from sklearn.cluster import KMeans\n",
    "centers = KMeans(n_clusters=n_centers).fit(Z).cluster_centers_\n",
    "centroid_mmd = CentroidMMDGMM(learner, adversary_g, kernel_fn, centers, np.ones(n_centers)*sigma)\n",
    "centroid_mmd.fit(Z_train, T_train, Y_train, learner_l2=learner_l2, adversary_l2=adversary_l2,\n",
    "                 adversary_norm_reg=adversary_norm_reg,\n",
    "                 learner_lr=learner_lr, adversary_lr=adversary_lr, n_epochs=n_epochs, bs=bs, logger=logger,\n",
    "                 model_dir='centroid_model', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(centroid_mmd, T_test_tens, G_test_tens, ind=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un-Regularized MMD-GMM with Kernel Loss and Learned Kernel\n",
    "\n",
    "We are minimizing the objective:\n",
    "\\begin{equation}\n",
    "\\min_{\\theta} \\max_{w} \\frac{1}{n^2} \\sum_{i,j} (y_i - h_{\\theta}(x_i)) K_{\\sigma}(g_w(z_i), g_w(z_j)) (y_j - h_{\\theta}(x_j)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger(learner, adversary, epoch, writer):\n",
    "    writer.add_histogram('learner', learner[-1].weight, epoch)\n",
    "    writer.add_histogram('adversary', adversary.sigma, epoch)\n",
    "    log_metrics(Z_val, T_val, Y_val, Z_val, T_val, Y_val, T_test_tens,\n",
    "                learner, adversary, epoch, writer, true_of_T=G_val, loss='kernel')\n",
    "\n",
    "kernelgmm = KernelLossAGMM(learner, adversary_g, kernel_fn, sigma)\n",
    "kernelgmm.fit(Z_train, T_train, Y_train, learner_l2=learner_l2**2, adversary_l2=adversary_l2,\n",
    "              learner_lr=learner_lr, adversary_lr=adversary_lr, n_epochs=n_epochs,\n",
    "              bs=bs, logger=logger, model_dir='kernel_model', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(kernelgmm, T_test_tens, G_test_tens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_2$-Regularized MMD-GMM without Kernel Approximation\n",
    "\n",
    "Here we test for function of the form:\n",
    "\\begin{equation}\n",
    "f(z) = \\sum_{j=1}^n \\beta_j K(g_w(z_j), g_w(z))\n",
    "\\end{equation}\n",
    "where $i$ ranges over all the training samples. Since the function itself depends on all the data, we need to create unbiased stochastic estimates of the test function at each time step, as well as unbiased stochastic estimtes of its RKHS and $\\ell_{2,n}$ norms. \n",
    "\n",
    "We sample a subset of the indices $S$ and test for:\n",
    "\\begin{equation}\n",
    "\\hat{f}(z) = \\frac{n}{|S|}\\sum_{j\\in S} \\beta_j K(g_w(z_i), g_w(z)).\n",
    "\\end{equation}\n",
    "Moreover, in order to create an unbiased estimate of the second moment of the test function on the training data, we also draw another set of indices $T$ and a minibatch of data $B$ and approximate it via: \n",
    "\\begin{equation}\n",
    "\\widehat{\\|f\\|_{2,n}^2} = \\frac{1}{|B|} \\frac{n}{|S|} \\frac{n}{|T|}\\sum_{i\\in B, j\\in S, k\\in T} \\beta_j \\beta_k K(g_w(z_j), g_w(z_i)) K(g_w(z_k), g_w(z_i)). \n",
    "\\end{equation}\n",
    "Moreover, an unbiased estimate of the RKHS norm of the function is created as:\n",
    "\\begin{equation}\n",
    "\\widehat{\\|f\\|_{K}^2} = \\frac{n}{|S|} \\frac{n}{|T|}\\sum_{j \\in S, k\\in T} \\beta_j \\beta_k K(g_w(z_j), g_w(z_k))\n",
    "\\end{equation}\n",
    "\n",
    "This might require some more fine tuning as the current version is too slow to learn. The gradients here have huge variance. We most probably need to sample the indices $j$ should be sampled proportional to $|\\beta_j|$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "def logger(learner, adversary, epoch, writer):\n",
    "    writer.add_histogram('learner', learner[-1].weight, epoch)\n",
    "    writer.add_histogram('adversary', adversary.sigma, epoch)\n",
    "    log_metrics(Z_val, T_val, Y_val, Z_val, T_val, Y_val, T_test_tens,\n",
    "                learner, adversary, epoch, writer, true_of_T=G_val, loss=None)\n",
    "\n",
    "mmdgmm = MMDGMM(learner, adversary_g, Y_train.shape[0], kernel_fn, sigma*np.ones(Y_train.shape[0]))\n",
    "mmdgmm.fit(Z_train, T_train, Y_train, learner_l2=learner_l2, adversary_l2=adversary_l2,\n",
    "           adversary_norm_reg=adversary_norm_reg,\n",
    "           learner_lr=learner_lr, adversary_lr=adversary_lr, n_epochs=n_epochs, bs1=bs, bs2=300, bs3=100,\n",
    "           logger=logger, model_dir='mmd_model', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(mmdgmm, T_test_tens, G_test_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetExamples.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
