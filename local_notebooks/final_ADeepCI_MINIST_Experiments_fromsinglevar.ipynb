{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d77d2b9-1d2b-4d84-8c0e-2b51d3a546e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import \n",
    "\n",
    "seedNum = 888\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seedNum)\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "np.random.seed(seedNum)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "#import autokeras as ak\n",
    "#import keras_tuner as kt\n",
    "\n",
    "\n",
    "#load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aaf8b10a-8dd2-42d5-9c51-7a12f027e2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train/255\n",
    "X_test  = X_test/255\n",
    "(example_X_train,example_y_train) = (X_train[:2000], y_train[:2000])\n",
    "(example_X_test,example_y_test) = (X_test[:2000], y_test[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fab882a9-0c11-4b14-8b59-c6e96ab30e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = y_train\n",
    "b = X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "200e8919-53b0-42e5-bdac-f85441a5b72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_run = \"none\"\n",
    "n_epochs = 60\n",
    "bs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8552fff8-2417-4e18-bcdb-f725e23ca923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "torch.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d331d3a7-f7c5-4fae-8334-4120b9635831",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function(func_str):\n",
    "    if func_str == \"abs\":\n",
    "        return (lambda x: (-1+0.4*np.abs(x)).flatten(), \n",
    "                lambda x: (-1+0.4*torch.abs(x)).flatten())\n",
    "    elif func_str == \"log\":\n",
    "        return (lambda x: 2*np.log(np.abs(x)).flatten(), \n",
    "                lambda x: 2*torch.log(torch.abs(x)).flatten())\n",
    "    elif func_str == \"sin\":\n",
    "        return (lambda x: (0.5+0.5*np.sin(x)).flatten(), \n",
    "                lambda x: (0.5+0.5*torch.sin(x)).flatten())\n",
    "    elif func_str == \"none\":\n",
    "        return (lambda x: 0.2*x.flatten(), \n",
    "                lambda x: 0.2*torch.Tensor(x).flatten())\n",
    "    else:\n",
    "        return (lambda x: np.sign(np.abs(np.abs(x)-5)-2).flatten(), \n",
    "                lambda x: torch.sign(torch.abs(torch.abs(x)-5)-2).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "afb4ee77-a552-4026-9dac-14abf4b70b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simdata(NUM_I,seed,func,rho): #a = y_train/test, b = X_train/test\n",
    "    \n",
    "    np.random.seed(seed)    \n",
    "    X_1_a_j = [] \n",
    "    X_2_a_j = [] \n",
    "    X_1_a_k = [] \n",
    "    X_2_a_k = [] \n",
    "    X_1_b_j = [] \n",
    "    X_2_b_j = [] \n",
    "    X_1_b_k = [] \n",
    "    X_2_b_k = []\n",
    "    Z       = []\n",
    "    \n",
    "\n",
    "    X_2_a_j_t = [] \n",
    "    X_2_a_k_t = [] \n",
    "    X_2_b_j_t = [] \n",
    "    X_2_b_k_t = []\n",
    "\n",
    "    for i in tqdm(range(0,NUM_I)):\n",
    "        J = np.random.randint(4,10) # number of choice\n",
    "              \n",
    "        \n",
    "        samplea = np.array(random.sample(list(np.arange(a.shape[0])),J)) # a list of index\n",
    "        samplea = np.expand_dims(samplea, axis=1)\n",
    "        ej = np.concatenate([a[i] for i in samplea],axis = None) # a list of number on images\n",
    "        ej = [i for i in ej.tolist()]\n",
    "        ej = np.float_(ej)\n",
    "\n",
    "        X_1_a = np.random.uniform(-1,1,J) #customer a\n",
    "        X_2_a = ej\n",
    "        X_2_a_pic = [torch.Tensor(b[i]) for i in samplea]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        sampleb = np.array(random.sample(list(np.arange(a.shape[0])),J)) # a list of index\n",
    "        sampleb = np.expand_dims(sampleb, axis=1)\n",
    "        ej = np.concatenate([a[i] for i in sampleb],axis = None) # a list of number on images\n",
    "        ej = [i for i in ej.tolist()]\n",
    "        ej = np.float_(ej)\n",
    "        \n",
    "        X_1_b = np.random.uniform(-1,1,J) #customer b\n",
    "        X_2_b = ej\n",
    "        X_2_b_pic = [torch.Tensor(b[i]) for i in sampleb]\n",
    "        \n",
    "        \n",
    "        \n",
    "        xi  = np.random.normal(0,0.5,J)    # same across all customers\n",
    "        \n",
    "        X_2_a = X_2_a #+ rho*xi  #customer a endogeneity remove for now\n",
    "        X_2_b = X_2_b #+ rho*xi  #customer b endogeneity remove for now\n",
    "        \n",
    "        u_a   = X_1_a + 2*func(X_2_a) + xi + np.random.normal(0,3,J) # \\epsilon_{a} # 3 for score\n",
    "        u_b   = X_1_b + 2*func(X_2_b) + xi + np.random.normal(0,3,J) # \\epsilon_{b}\n",
    "        \n",
    "        choice_j = np.argmax(u_a) # return the index of product in the sample that customer a chose, we assume customer a as choose j\n",
    "        choice_k = np.argmax(u_b) # return the index of product in the sample that customer b chose, we assume customer b as choose k\n",
    "               \n",
    "        if choice_j == choice_k:\n",
    "            continue\n",
    "        else:  \n",
    "\n",
    "            X_1_a_j.append(X_1_a[choice_j])\n",
    "            X_2_a_j.append(X_2_a_pic[choice_j])\n",
    "            X_1_a_k.append(X_1_a[choice_k])\n",
    "            X_2_a_k.append(X_2_a_pic[choice_k])\n",
    "            \n",
    "            X_1_b_j.append(X_1_b[choice_j]) \n",
    "            X_2_b_j.append(X_2_b_pic[choice_j]) \n",
    "            X_1_b_k.append(X_1_b[choice_k]) \n",
    "            X_2_b_k.append(X_2_b_pic[choice_k])\n",
    "            \n",
    "            \n",
    "            \n",
    "            X_2_a_j_t.append(X_2_a[choice_j])\n",
    "            X_2_a_k_t.append(X_2_a[choice_k])             \n",
    "            X_2_b_j_t.append(X_2_b[choice_j]) \n",
    "            X_2_b_k_t.append(X_2_b[choice_k])\n",
    "            #Z.append(np.array([X_1_a[choice_j], X_2_a[choice_j], X_1_a[choice_k], X_2_a[choice_k],X_1_b[choice_j], X_2_b[choice_j], X_1_b[choice_k], X_2_b[choice_k]]))\n",
    "            Z.append(np.array([X_1_a[choice_j],X_1_a[choice_k],X_1_b[choice_j],X_1_b[choice_k]]))\n",
    "            \n",
    "    X_2_a_j = torch.cat(X_2_a_j, out=torch.Tensor(len(X_2_a_j), 28, 28))\n",
    "    X_2_a_k = torch.cat(X_2_a_k, out=torch.Tensor(len(X_2_a_k), 28, 28))\n",
    "    X_2_b_j = torch.cat(X_2_b_j, out=torch.Tensor(len(X_2_b_j), 28, 28))\n",
    "    X_2_b_k = torch.cat(X_2_b_k, out=torch.Tensor(len(X_2_b_k), 28, 28))\n",
    "\n",
    "    #X_1_a_j = torch.Tensor(X_1_a_j)\n",
    "    #X_1_a_k = torch.Tensor(X_1_a_k)\n",
    "    #X_1_b_j = torch.Tensor(X_1_b_j)\n",
    "    #X_1_b_k = torch.Tensor(X_1_b_k)\n",
    "            \n",
    "\n",
    "    \n",
    "    return torch.Tensor(X_1_a_j).reshape((-1,1)).double(), X_2_a_j.unsqueeze(1), \\\n",
    "torch.Tensor(X_1_a_k).reshape((-1,1)).double(), X_2_a_k.unsqueeze(1), torch.Tensor(X_1_b_j).reshape((-1,1)).double(), \\\n",
    "X_2_b_j.unsqueeze(1), torch.Tensor(X_1_b_k).reshape((-1,1)).double(), X_2_b_k.unsqueeze(1), \\\n",
    "torch.tensor(Z, dtype=torch.float64),\\\n",
    "torch.Tensor(X_2_a_j_t).reshape((-1,1)).unsqueeze(1).double(), torch.Tensor(X_2_a_k_t).reshape((-1,1)).unsqueeze(1).double(),\\\n",
    "torch.Tensor(X_2_b_j_t).reshape((-1,1)).unsqueeze(1).double(), torch.Tensor(X_2_b_k_t).reshape((-1,1)).unsqueeze(1).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "16ad8027-3000-4929-b8d3-47a42661db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa,bb = get_function(func_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a38b07e4-f0c0-4b72-96d6-925250d5953a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:40<00:00, 123.37it/s]\n"
     ]
    }
   ],
   "source": [
    "X_1_a_j, X_2_a_j, X_1_a_k, X_2_a_k, X_1_b_j, X_2_b_j, X_1_b_k, X_2_b_k, Z,  X_2_a_j_t, X_2_a_k_t,  X_2_b_j_t,  X_2_b_k_t= Simdata(5000,2,aa,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "af377986-f660-44fb-bebf-196143c5df74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4136, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "image = X_2_a_k[6]\n",
    "# plot the sample\n",
    "fig = plt.figure\n",
    "plt.imshow(image.squeeze(0), cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "print(X_2_a_k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "93656844-b6ad-4614-ae56-82e55e9432f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "95b4d8d7-1c6f-4b3f-bb7d-cfcc9124013c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/notebooks/AdversarialGMM/local_notebooks', '/usr/lib/python39.zip', '/usr/lib/python3.9', '/usr/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/dist-packages', '/usr/lib/python3/dist-packages', '/notebooks/AdversarialGMM/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path\n",
    "print(sys.path)\n",
    "sys.path.append('/notebooks/AdversarialGMM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fd8f6a60-b62f-4413-9b13-2696b09808af",
   "metadata": {},
   "outputs": [],
   "source": [
    "### module imports\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "### import from our files\n",
    "from mliv.dgps import get_data, get_tau_fn, fn_dict\n",
    "from mliv.neuralnet.utilities import log_metrics, plot_results, hyperparam_grid,\\\n",
    "                                     hyperparam_mult_grid, eval_performance\n",
    "from mliv.neuralnet.mnist_dgps import AbstractMNISTxz\n",
    "from mliv.neuralnet import AGMM,KernelLayerMMDGMM\n",
    "from mliv.neuralnet.rbflayer import gaussian, inverse_multiquadric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "884b4136-77ea-4c22-8e12-857fe1d8954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f1768071-a988-45df-a18e-13de550552a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2a82e63c-963d-4dec-9ace-4f6d403f0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Z_agmm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Z_agmm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x  # F.log_softmax(x, dim=1)\n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "class CNN_Z_kernel(nn.Module):\n",
    "    def __init__(self, g_features=100):\n",
    "        super(CNN_Z_kernel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, g_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x  # F.log_softmax(x, dim=1)\n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "class CNN_X(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_X, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "        self.finallayer = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.finallayer(x)\n",
    "        output = x #F.tanh(x) #F.softmax(x, dim=1) #x\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e893ec27-77fd-4945-a9ee-f4ae0fb6a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_z_kernel(n_z, n_hidden, g_features, dropout_p):\n",
    "    FC_Z_kernel = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_z, n_hidden),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_hidden, g_features),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    return FC_Z_kernel\n",
    "\n",
    "\n",
    "def fc_z_agmm(n_z, n_hidden, dropout_p):\n",
    "    FC_Z_agmm = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_z, n_hidden),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_hidden, 1),\n",
    "    )\n",
    "    return FC_Z_agmm\n",
    "\n",
    "\n",
    "def fc_x(n_t, n_hidden, dropout_p):\n",
    "    FC_X = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_t, n_hidden),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_hidden, 1),\n",
    "    )\n",
    "    return FC_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef6c0059-d68e-48c8-a917-13f46b58f923",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms, utils, models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5aab8430-e3fa-4560-a3da-8e0178dbdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "n_hidden = 300\n",
    "n_instruments = 1\n",
    "dropout_p = 0.1\n",
    "\n",
    "net_learner = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, k),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(k, 200),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(200, 1),\n",
    "            )\n",
    "\n",
    "#learner = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_t, n_hidden), nn.LeakyReLU(),\n",
    "#                        nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
    "#                        nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
    "\n",
    "#adversary_fn = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_z, n_hidden), nn.LeakyReLU(),\n",
    "#                             nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
    "#                             nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
    "\n",
    "net_adversary = torch.nn.Sequential(nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(4, k),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),                        \n",
    "            torch.nn.Linear(k, 200), #200\n",
    "            torch.nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),                        \n",
    "            torch.nn.Linear(200, 1),\n",
    "            )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f3a26a7-179c-4b94-a666-2c6a40f03131",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "resnet50 = torch.hub.load('pytorch/vision:v0.10.0', 'resnet50', pretrained=True)\n",
    "resnet50.fc = nn.Identity()\n",
    "class Net(nn.Module):\n",
    "    #This defines the structure of the NN.\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.resnet = resnet50\n",
    "        self.fc1 = nn.Linear(2048, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #Convolutional Layer/Pooling Layer/Activation\n",
    "        x = self.resnet(x) \n",
    "        #Convolutional Layer/Dropout/Pooling Layer/Activation\n",
    "        #Fully Connected Layer/Activation\n",
    "        x = self.fc1(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7ec5d7a-a5cd-4ef3-a72e-0387065fa0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args={}\n",
    "kwargs={}\n",
    "args['batch_size']=100\n",
    "args['test_batch_size']=1000\n",
    "args['epochs']=10  #The number of Epochs is the number of times you go through the full dataset. \n",
    "args['lr']=0.0005 #Learning rate is how fast it will decend. \n",
    "args['momentum']=0.5 #SGD momentum (default: 0.5) Momentum is a moving average of our gradients (helps to keep direction).\n",
    "\n",
    "args['seed']=1 #random seed\n",
    "args['log_interval']=10\n",
    "args['cuda']=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af8e7340-744a-419f-ba28-ed4aa0ada630",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       \n",
    "                   ])),\n",
    "    batch_size=args['batch_size'], shuffle=True, **kwargs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       #transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args['test_batch_size'], shuffle=True, **kwargs)\n",
    "\n",
    "import torchvision.transforms as T\n",
    "transform = T.Resize((32,32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfb36100-e368-4ba3-b0f2-1df71249c260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        #Variables in Pytorch are differenciable. \n",
    "        #data = transform(data)\n",
    "        #data = torch.concat([data,data,data],axis=1)\n",
    "        #\n",
    "        data = data.float()\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        #This will zero out the gradients for this batch. \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        # Calculate the loss The negative log likelihood loss. It is useful to train a classification problem with C classes.\n",
    "        loss = F.mse_loss(output.float(), target.float())\n",
    "        #dloss/dx for every Variable \n",
    "        loss.backward()\n",
    "        #to do a one-step update on our parameter.\n",
    "        optimizer.step()\n",
    "        #Print out the loss periodically. \n",
    "        if batch_idx % args['log_interval'] == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss))\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if args['cuda']:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        data = transform(data)\n",
    "        data = torch.concat([data,data,data],axis=1)\n",
    "        data = data.float()\n",
    "        output = model(data)\n",
    "        test_loss += F.mse_loss(output, target/10)\n",
    "        \n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}\\n'.format(\n",
    "        test_loss, ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "988ed727-a48d-4d1e-babb-0a3bf9ce94a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net_X(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net_X, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        # output = F.log_softmax(x, dim=1)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "656538b7-b70d-4f25-a3d6-639be94476de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net_X(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (dropout2): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=9216, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Net_X()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d9eabc04-ea63-4e93-a804-8fe3bd4833bf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 28.328876\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 14.382548\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 10.151765\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 8.354701\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 10.657713\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 7.117265\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 7.295788\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 7.101282\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 6.988516\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 6.065816\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 5.429741\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 5.291254\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 4.465590\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 4.126276\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 4.164833\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 4.975183\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 5.118373\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 3.964350\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 4.224464\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 5.165137\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 3.625266\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 4.956895\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 4.802920\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 4.359973\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 4.065035\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 3.067546\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 3.297285\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 3.839571\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 4.199298\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 3.386365\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 3.276665\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 3.505621\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 3.036608\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 2.912259\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 3.147714\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 3.185002\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 3.716970\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 4.007143\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 2.621643\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 3.308718\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 3.120756\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 3.192069\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 4.623498\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 2.558872\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 2.611638\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 2.599887\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 2.551844\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 3.039895\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.798059\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 2.971537\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.298528\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 4.208896\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 2.698606\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 2.869029\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 2.829935\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 2.470502\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 2.296610\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 3.449058\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 2.464738\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 2.466259\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 3.071060\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 2.419695\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 2.247855\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 2.205781\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 2.518352\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 1.866501\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 2.231143\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 1.639536\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 3.359169\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 2.666509\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.092028\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 2.170401\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 2.305435\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: 2.487807\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 2.488761\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 2.534221\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 1.562571\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: 2.249605\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 2.353304\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: 2.967261\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.156625\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 2.674892\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 2.584721\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: 2.187159\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 2.096015\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 2.499577\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 2.598376\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 2.423810\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 2.809914\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: 1.891189\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.134017\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: 2.530327\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.221483\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 2.585036\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 1.786734\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 2.586408\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 1.913835\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: 3.226207\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 1.310547\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 2.067419\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.030427\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: 1.823031\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 1.700006\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: 2.008790\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 1.903167\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 2.642943\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 2.493769\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: 1.684953\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 2.705214\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: 2.123490\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 1.609290\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 3.644678\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 2.148011\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: 2.307331\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 2.329330\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 2.015718\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 2.550278\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 1.841105\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 2.254237\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: 1.954089\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.519578\n",
      "Train Epoch: 3 [1000/60000 (2%)]\tLoss: 1.619144\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 2.085846\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: 1.963572\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 2.568823\n",
      "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 2.523432\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 2.390000\n",
      "Train Epoch: 3 [7000/60000 (12%)]\tLoss: 1.960715\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 2.147308\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: 2.418422\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 2.321724\n",
      "Train Epoch: 3 [11000/60000 (18%)]\tLoss: 1.501025\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 2.125801\n",
      "Train Epoch: 3 [13000/60000 (22%)]\tLoss: 1.914823\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 1.745204\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 2.381365\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 2.536974\n",
      "Train Epoch: 3 [17000/60000 (28%)]\tLoss: 2.474282\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 2.582510\n",
      "Train Epoch: 3 [19000/60000 (32%)]\tLoss: 2.981141\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 1.976982\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: 1.855351\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 2.649067\n",
      "Train Epoch: 3 [23000/60000 (38%)]\tLoss: 1.599859\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 1.152014\n",
      "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 2.188422\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 1.526460\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: 1.898104\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 2.163120\n",
      "Train Epoch: 3 [29000/60000 (48%)]\tLoss: 1.752361\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 1.771472\n",
      "Train Epoch: 3 [31000/60000 (52%)]\tLoss: 2.591750\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.161592\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: 1.685853\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 2.262960\n",
      "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 2.199929\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 2.320014\n",
      "Train Epoch: 3 [37000/60000 (62%)]\tLoss: 1.292351\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 2.010577\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: 2.458220\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 1.918751\n",
      "Train Epoch: 3 [41000/60000 (68%)]\tLoss: 1.321336\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 2.188680\n",
      "Train Epoch: 3 [43000/60000 (72%)]\tLoss: 2.071656\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 2.141517\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 2.055827\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 1.855047\n",
      "Train Epoch: 3 [47000/60000 (78%)]\tLoss: 1.721682\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 2.585914\n",
      "Train Epoch: 3 [49000/60000 (82%)]\tLoss: 2.540726\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 1.749412\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: 2.782328\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 2.057236\n",
      "Train Epoch: 3 [53000/60000 (88%)]\tLoss: 2.473840\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 2.331189\n",
      "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 1.730971\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 2.031274\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: 1.648685\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 2.209959\n",
      "Train Epoch: 3 [59000/60000 (98%)]\tLoss: 2.180225\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 1.536009\n",
      "Train Epoch: 4 [1000/60000 (2%)]\tLoss: 2.160147\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 2.344770\n",
      "Train Epoch: 4 [3000/60000 (5%)]\tLoss: 2.149779\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 1.819015\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 2.292171\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 2.263909\n",
      "Train Epoch: 4 [7000/60000 (12%)]\tLoss: 1.808062\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 1.742079\n",
      "Train Epoch: 4 [9000/60000 (15%)]\tLoss: 1.942262\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 1.958534\n",
      "Train Epoch: 4 [11000/60000 (18%)]\tLoss: 1.392590\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 1.975581\n",
      "Train Epoch: 4 [13000/60000 (22%)]\tLoss: 1.652955\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 1.633092\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 1.736911\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 2.051937\n",
      "Train Epoch: 4 [17000/60000 (28%)]\tLoss: 1.889868\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 2.074169\n",
      "Train Epoch: 4 [19000/60000 (32%)]\tLoss: 1.612045\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 1.700485\n",
      "Train Epoch: 4 [21000/60000 (35%)]\tLoss: 1.575258\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 1.030442\n",
      "Train Epoch: 4 [23000/60000 (38%)]\tLoss: 1.380586\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 1.351466\n",
      "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 1.905770\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 1.982683\n",
      "Train Epoch: 4 [27000/60000 (45%)]\tLoss: 2.404226\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 1.596759\n",
      "Train Epoch: 4 [29000/60000 (48%)]\tLoss: 1.899358\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 1.750623\n",
      "Train Epoch: 4 [31000/60000 (52%)]\tLoss: 1.908525\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 1.622632\n",
      "Train Epoch: 4 [33000/60000 (55%)]\tLoss: 1.778067\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 1.663228\n",
      "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 1.997221\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 1.276895\n",
      "Train Epoch: 4 [37000/60000 (62%)]\tLoss: 2.230629\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 1.616060\n",
      "Train Epoch: 4 [39000/60000 (65%)]\tLoss: 2.074991\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.452918\n",
      "Train Epoch: 4 [41000/60000 (68%)]\tLoss: 2.350703\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 1.759102\n",
      "Train Epoch: 4 [43000/60000 (72%)]\tLoss: 1.398869\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 1.560670\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 1.928115\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 1.919593\n",
      "Train Epoch: 4 [47000/60000 (78%)]\tLoss: 1.324677\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 1.282877\n",
      "Train Epoch: 4 [49000/60000 (82%)]\tLoss: 1.861634\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 1.655329\n",
      "Train Epoch: 4 [51000/60000 (85%)]\tLoss: 1.424563\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 2.244745\n",
      "Train Epoch: 4 [53000/60000 (88%)]\tLoss: 1.454913\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 2.166090\n",
      "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 1.942173\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.978615\n",
      "Train Epoch: 4 [57000/60000 (95%)]\tLoss: 1.554226\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 1.808344\n",
      "Train Epoch: 4 [59000/60000 (98%)]\tLoss: 1.907112\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 1.627984\n",
      "Train Epoch: 5 [1000/60000 (2%)]\tLoss: 1.747859\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 1.148329\n",
      "Train Epoch: 5 [3000/60000 (5%)]\tLoss: 2.542398\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 1.552059\n",
      "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 1.890492\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 1.948970\n",
      "Train Epoch: 5 [7000/60000 (12%)]\tLoss: 1.693701\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 1.351080\n",
      "Train Epoch: 5 [9000/60000 (15%)]\tLoss: 1.788257\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 2.399307\n",
      "Train Epoch: 5 [11000/60000 (18%)]\tLoss: 1.670155\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 2.122915\n",
      "Train Epoch: 5 [13000/60000 (22%)]\tLoss: 2.361132\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 2.271952\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 1.403819\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 1.804567\n",
      "Train Epoch: 5 [17000/60000 (28%)]\tLoss: 1.933574\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 1.614745\n",
      "Train Epoch: 5 [19000/60000 (32%)]\tLoss: 1.870679\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 1.533025\n",
      "Train Epoch: 5 [21000/60000 (35%)]\tLoss: 1.716225\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 2.166704\n",
      "Train Epoch: 5 [23000/60000 (38%)]\tLoss: 1.470052\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 2.163531\n",
      "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 2.000610\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 1.898682\n",
      "Train Epoch: 5 [27000/60000 (45%)]\tLoss: 2.003924\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 1.899459\n",
      "Train Epoch: 5 [29000/60000 (48%)]\tLoss: 1.462892\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 2.252562\n",
      "Train Epoch: 5 [31000/60000 (52%)]\tLoss: 1.787587\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 1.736873\n",
      "Train Epoch: 5 [33000/60000 (55%)]\tLoss: 1.955637\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 1.551280\n",
      "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 1.689064\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 1.698189\n",
      "Train Epoch: 5 [37000/60000 (62%)]\tLoss: 1.634933\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 1.835862\n",
      "Train Epoch: 5 [39000/60000 (65%)]\tLoss: 1.629246\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 1.699108\n",
      "Train Epoch: 5 [41000/60000 (68%)]\tLoss: 2.204722\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 1.808872\n",
      "Train Epoch: 5 [43000/60000 (72%)]\tLoss: 1.705207\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 1.475453\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 1.966769\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 1.841990\n",
      "Train Epoch: 5 [47000/60000 (78%)]\tLoss: 2.017976\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 1.502517\n",
      "Train Epoch: 5 [49000/60000 (82%)]\tLoss: 2.008332\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 1.481404\n",
      "Train Epoch: 5 [51000/60000 (85%)]\tLoss: 0.930466\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 1.478727\n",
      "Train Epoch: 5 [53000/60000 (88%)]\tLoss: 1.747892\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 1.668390\n",
      "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 1.630765\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 1.581511\n",
      "Train Epoch: 5 [57000/60000 (95%)]\tLoss: 2.017873\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 2.095344\n",
      "Train Epoch: 5 [59000/60000 (98%)]\tLoss: 1.814110\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 1.261585\n",
      "Train Epoch: 6 [1000/60000 (2%)]\tLoss: 1.410354\n",
      "Train Epoch: 6 [2000/60000 (3%)]\tLoss: 1.516145\n",
      "Train Epoch: 6 [3000/60000 (5%)]\tLoss: 1.284908\n",
      "Train Epoch: 6 [4000/60000 (7%)]\tLoss: 1.811090\n",
      "Train Epoch: 6 [5000/60000 (8%)]\tLoss: 2.197830\n",
      "Train Epoch: 6 [6000/60000 (10%)]\tLoss: 1.362352\n",
      "Train Epoch: 6 [7000/60000 (12%)]\tLoss: 1.218765\n",
      "Train Epoch: 6 [8000/60000 (13%)]\tLoss: 1.382271\n",
      "Train Epoch: 6 [9000/60000 (15%)]\tLoss: 1.456696\n",
      "Train Epoch: 6 [10000/60000 (17%)]\tLoss: 1.313963\n",
      "Train Epoch: 6 [11000/60000 (18%)]\tLoss: 1.667243\n",
      "Train Epoch: 6 [12000/60000 (20%)]\tLoss: 1.296932\n",
      "Train Epoch: 6 [13000/60000 (22%)]\tLoss: 1.998383\n",
      "Train Epoch: 6 [14000/60000 (23%)]\tLoss: 1.963482\n",
      "Train Epoch: 6 [15000/60000 (25%)]\tLoss: 2.258358\n",
      "Train Epoch: 6 [16000/60000 (27%)]\tLoss: 1.434517\n",
      "Train Epoch: 6 [17000/60000 (28%)]\tLoss: 1.031762\n",
      "Train Epoch: 6 [18000/60000 (30%)]\tLoss: 1.505481\n",
      "Train Epoch: 6 [19000/60000 (32%)]\tLoss: 1.682300\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 2.244866\n",
      "Train Epoch: 6 [21000/60000 (35%)]\tLoss: 1.372493\n",
      "Train Epoch: 6 [22000/60000 (37%)]\tLoss: 1.406702\n",
      "Train Epoch: 6 [23000/60000 (38%)]\tLoss: 1.390549\n",
      "Train Epoch: 6 [24000/60000 (40%)]\tLoss: 1.473667\n",
      "Train Epoch: 6 [25000/60000 (42%)]\tLoss: 1.769147\n",
      "Train Epoch: 6 [26000/60000 (43%)]\tLoss: 2.122129\n",
      "Train Epoch: 6 [27000/60000 (45%)]\tLoss: 2.076204\n",
      "Train Epoch: 6 [28000/60000 (47%)]\tLoss: 1.581681\n",
      "Train Epoch: 6 [29000/60000 (48%)]\tLoss: 2.058234\n",
      "Train Epoch: 6 [30000/60000 (50%)]\tLoss: 1.339532\n",
      "Train Epoch: 6 [31000/60000 (52%)]\tLoss: 1.407223\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.826998\n",
      "Train Epoch: 6 [33000/60000 (55%)]\tLoss: 1.350767\n",
      "Train Epoch: 6 [34000/60000 (57%)]\tLoss: 1.830966\n",
      "Train Epoch: 6 [35000/60000 (58%)]\tLoss: 1.680427\n",
      "Train Epoch: 6 [36000/60000 (60%)]\tLoss: 1.030338\n",
      "Train Epoch: 6 [37000/60000 (62%)]\tLoss: 1.935793\n",
      "Train Epoch: 6 [38000/60000 (63%)]\tLoss: 1.458010\n",
      "Train Epoch: 6 [39000/60000 (65%)]\tLoss: 1.538257\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 1.346063\n",
      "Train Epoch: 6 [41000/60000 (68%)]\tLoss: 1.556443\n",
      "Train Epoch: 6 [42000/60000 (70%)]\tLoss: 1.442532\n",
      "Train Epoch: 6 [43000/60000 (72%)]\tLoss: 1.912525\n",
      "Train Epoch: 6 [44000/60000 (73%)]\tLoss: 2.127542\n",
      "Train Epoch: 6 [45000/60000 (75%)]\tLoss: 1.399518\n",
      "Train Epoch: 6 [46000/60000 (77%)]\tLoss: 1.551132\n",
      "Train Epoch: 6 [47000/60000 (78%)]\tLoss: 1.206214\n",
      "Train Epoch: 6 [48000/60000 (80%)]\tLoss: 2.025918\n",
      "Train Epoch: 6 [49000/60000 (82%)]\tLoss: 1.232337\n",
      "Train Epoch: 6 [50000/60000 (83%)]\tLoss: 1.500417\n",
      "Train Epoch: 6 [51000/60000 (85%)]\tLoss: 1.649148\n",
      "Train Epoch: 6 [52000/60000 (87%)]\tLoss: 1.475052\n",
      "Train Epoch: 6 [53000/60000 (88%)]\tLoss: 2.078110\n",
      "Train Epoch: 6 [54000/60000 (90%)]\tLoss: 1.759650\n",
      "Train Epoch: 6 [55000/60000 (92%)]\tLoss: 1.778749\n",
      "Train Epoch: 6 [56000/60000 (93%)]\tLoss: 1.595517\n",
      "Train Epoch: 6 [57000/60000 (95%)]\tLoss: 2.040119\n",
      "Train Epoch: 6 [58000/60000 (97%)]\tLoss: 1.961901\n",
      "Train Epoch: 6 [59000/60000 (98%)]\tLoss: 1.261269\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 1.872189\n",
      "Train Epoch: 7 [1000/60000 (2%)]\tLoss: 2.170279\n",
      "Train Epoch: 7 [2000/60000 (3%)]\tLoss: 1.161640\n",
      "Train Epoch: 7 [3000/60000 (5%)]\tLoss: 1.735818\n",
      "Train Epoch: 7 [4000/60000 (7%)]\tLoss: 1.819978\n",
      "Train Epoch: 7 [5000/60000 (8%)]\tLoss: 1.961224\n",
      "Train Epoch: 7 [6000/60000 (10%)]\tLoss: 0.856071\n",
      "Train Epoch: 7 [7000/60000 (12%)]\tLoss: 1.242525\n",
      "Train Epoch: 7 [8000/60000 (13%)]\tLoss: 1.583891\n",
      "Train Epoch: 7 [9000/60000 (15%)]\tLoss: 1.402764\n",
      "Train Epoch: 7 [10000/60000 (17%)]\tLoss: 1.663459\n",
      "Train Epoch: 7 [11000/60000 (18%)]\tLoss: 1.876837\n",
      "Train Epoch: 7 [12000/60000 (20%)]\tLoss: 1.295597\n",
      "Train Epoch: 7 [13000/60000 (22%)]\tLoss: 1.406769\n",
      "Train Epoch: 7 [14000/60000 (23%)]\tLoss: 1.379951\n",
      "Train Epoch: 7 [15000/60000 (25%)]\tLoss: 1.804906\n",
      "Train Epoch: 7 [16000/60000 (27%)]\tLoss: 1.679811\n",
      "Train Epoch: 7 [17000/60000 (28%)]\tLoss: 1.270821\n",
      "Train Epoch: 7 [18000/60000 (30%)]\tLoss: 1.417588\n",
      "Train Epoch: 7 [19000/60000 (32%)]\tLoss: 1.290813\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 1.325684\n",
      "Train Epoch: 7 [21000/60000 (35%)]\tLoss: 1.995153\n",
      "Train Epoch: 7 [22000/60000 (37%)]\tLoss: 1.298256\n",
      "Train Epoch: 7 [23000/60000 (38%)]\tLoss: 1.787587\n",
      "Train Epoch: 7 [24000/60000 (40%)]\tLoss: 1.810406\n",
      "Train Epoch: 7 [25000/60000 (42%)]\tLoss: 1.469097\n",
      "Train Epoch: 7 [26000/60000 (43%)]\tLoss: 2.408389\n",
      "Train Epoch: 7 [27000/60000 (45%)]\tLoss: 1.139736\n",
      "Train Epoch: 7 [28000/60000 (47%)]\tLoss: 1.199921\n",
      "Train Epoch: 7 [29000/60000 (48%)]\tLoss: 1.233803\n",
      "Train Epoch: 7 [30000/60000 (50%)]\tLoss: 1.796566\n",
      "Train Epoch: 7 [31000/60000 (52%)]\tLoss: 1.571848\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 1.158769\n",
      "Train Epoch: 7 [33000/60000 (55%)]\tLoss: 1.222344\n",
      "Train Epoch: 7 [34000/60000 (57%)]\tLoss: 1.222481\n",
      "Train Epoch: 7 [35000/60000 (58%)]\tLoss: 1.079339\n",
      "Train Epoch: 7 [36000/60000 (60%)]\tLoss: 1.537761\n",
      "Train Epoch: 7 [37000/60000 (62%)]\tLoss: 2.053696\n",
      "Train Epoch: 7 [38000/60000 (63%)]\tLoss: 1.330940\n",
      "Train Epoch: 7 [39000/60000 (65%)]\tLoss: 1.410034\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 1.095546\n",
      "Train Epoch: 7 [41000/60000 (68%)]\tLoss: 1.799027\n",
      "Train Epoch: 7 [42000/60000 (70%)]\tLoss: 1.575001\n",
      "Train Epoch: 7 [43000/60000 (72%)]\tLoss: 1.624258\n",
      "Train Epoch: 7 [44000/60000 (73%)]\tLoss: 1.095942\n",
      "Train Epoch: 7 [45000/60000 (75%)]\tLoss: 1.582967\n",
      "Train Epoch: 7 [46000/60000 (77%)]\tLoss: 1.402536\n",
      "Train Epoch: 7 [47000/60000 (78%)]\tLoss: 1.492762\n",
      "Train Epoch: 7 [48000/60000 (80%)]\tLoss: 1.394178\n",
      "Train Epoch: 7 [49000/60000 (82%)]\tLoss: 2.005020\n",
      "Train Epoch: 7 [50000/60000 (83%)]\tLoss: 1.370006\n",
      "Train Epoch: 7 [51000/60000 (85%)]\tLoss: 1.289829\n",
      "Train Epoch: 7 [52000/60000 (87%)]\tLoss: 1.523783\n",
      "Train Epoch: 7 [53000/60000 (88%)]\tLoss: 2.030902\n",
      "Train Epoch: 7 [54000/60000 (90%)]\tLoss: 1.091159\n",
      "Train Epoch: 7 [55000/60000 (92%)]\tLoss: 2.087685\n",
      "Train Epoch: 7 [56000/60000 (93%)]\tLoss: 1.915125\n",
      "Train Epoch: 7 [57000/60000 (95%)]\tLoss: 1.324573\n",
      "Train Epoch: 7 [58000/60000 (97%)]\tLoss: 1.594494\n",
      "Train Epoch: 7 [59000/60000 (98%)]\tLoss: 1.738921\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 1.573845\n",
      "Train Epoch: 8 [1000/60000 (2%)]\tLoss: 0.946801\n",
      "Train Epoch: 8 [2000/60000 (3%)]\tLoss: 0.850768\n",
      "Train Epoch: 8 [3000/60000 (5%)]\tLoss: 0.966533\n",
      "Train Epoch: 8 [4000/60000 (7%)]\tLoss: 1.759398\n",
      "Train Epoch: 8 [5000/60000 (8%)]\tLoss: 1.855953\n",
      "Train Epoch: 8 [6000/60000 (10%)]\tLoss: 1.772522\n",
      "Train Epoch: 8 [7000/60000 (12%)]\tLoss: 1.482737\n",
      "Train Epoch: 8 [8000/60000 (13%)]\tLoss: 1.199600\n",
      "Train Epoch: 8 [9000/60000 (15%)]\tLoss: 1.160808\n",
      "Train Epoch: 8 [10000/60000 (17%)]\tLoss: 1.014416\n",
      "Train Epoch: 8 [11000/60000 (18%)]\tLoss: 0.945368\n",
      "Train Epoch: 8 [12000/60000 (20%)]\tLoss: 1.380808\n",
      "Train Epoch: 8 [13000/60000 (22%)]\tLoss: 1.314654\n",
      "Train Epoch: 8 [14000/60000 (23%)]\tLoss: 1.315279\n",
      "Train Epoch: 8 [15000/60000 (25%)]\tLoss: 1.345895\n",
      "Train Epoch: 8 [16000/60000 (27%)]\tLoss: 1.543663\n",
      "Train Epoch: 8 [17000/60000 (28%)]\tLoss: 1.481497\n",
      "Train Epoch: 8 [18000/60000 (30%)]\tLoss: 1.664599\n",
      "Train Epoch: 8 [19000/60000 (32%)]\tLoss: 2.024131\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 1.283264\n",
      "Train Epoch: 8 [21000/60000 (35%)]\tLoss: 1.375013\n",
      "Train Epoch: 8 [22000/60000 (37%)]\tLoss: 1.479449\n",
      "Train Epoch: 8 [23000/60000 (38%)]\tLoss: 1.636320\n",
      "Train Epoch: 8 [24000/60000 (40%)]\tLoss: 1.563861\n",
      "Train Epoch: 8 [25000/60000 (42%)]\tLoss: 1.514689\n",
      "Train Epoch: 8 [26000/60000 (43%)]\tLoss: 1.517125\n",
      "Train Epoch: 8 [27000/60000 (45%)]\tLoss: 1.494112\n",
      "Train Epoch: 8 [28000/60000 (47%)]\tLoss: 1.207544\n",
      "Train Epoch: 8 [29000/60000 (48%)]\tLoss: 1.776790\n",
      "Train Epoch: 8 [30000/60000 (50%)]\tLoss: 1.915851\n",
      "Train Epoch: 8 [31000/60000 (52%)]\tLoss: 1.735979\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 1.294810\n",
      "Train Epoch: 8 [33000/60000 (55%)]\tLoss: 0.903138\n",
      "Train Epoch: 8 [34000/60000 (57%)]\tLoss: 1.424454\n",
      "Train Epoch: 8 [35000/60000 (58%)]\tLoss: 1.516240\n",
      "Train Epoch: 8 [36000/60000 (60%)]\tLoss: 1.662374\n",
      "Train Epoch: 8 [37000/60000 (62%)]\tLoss: 1.783441\n",
      "Train Epoch: 8 [38000/60000 (63%)]\tLoss: 1.543413\n",
      "Train Epoch: 8 [39000/60000 (65%)]\tLoss: 1.807164\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.083136\n",
      "Train Epoch: 8 [41000/60000 (68%)]\tLoss: 1.516310\n",
      "Train Epoch: 8 [42000/60000 (70%)]\tLoss: 1.537010\n",
      "Train Epoch: 8 [43000/60000 (72%)]\tLoss: 0.998849\n",
      "Train Epoch: 8 [44000/60000 (73%)]\tLoss: 1.021024\n",
      "Train Epoch: 8 [45000/60000 (75%)]\tLoss: 1.225977\n",
      "Train Epoch: 8 [46000/60000 (77%)]\tLoss: 1.663026\n",
      "Train Epoch: 8 [47000/60000 (78%)]\tLoss: 1.478584\n",
      "Train Epoch: 8 [48000/60000 (80%)]\tLoss: 1.666407\n",
      "Train Epoch: 8 [49000/60000 (82%)]\tLoss: 1.160538\n",
      "Train Epoch: 8 [50000/60000 (83%)]\tLoss: 1.548877\n",
      "Train Epoch: 8 [51000/60000 (85%)]\tLoss: 3.086349\n",
      "Train Epoch: 8 [52000/60000 (87%)]\tLoss: 1.632569\n",
      "Train Epoch: 8 [53000/60000 (88%)]\tLoss: 1.440310\n",
      "Train Epoch: 8 [54000/60000 (90%)]\tLoss: 1.868447\n",
      "Train Epoch: 8 [55000/60000 (92%)]\tLoss: 1.207056\n",
      "Train Epoch: 8 [56000/60000 (93%)]\tLoss: 1.431519\n",
      "Train Epoch: 8 [57000/60000 (95%)]\tLoss: 1.647942\n",
      "Train Epoch: 8 [58000/60000 (97%)]\tLoss: 1.314204\n",
      "Train Epoch: 8 [59000/60000 (98%)]\tLoss: 1.373578\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 1.175329\n",
      "Train Epoch: 9 [1000/60000 (2%)]\tLoss: 1.358695\n",
      "Train Epoch: 9 [2000/60000 (3%)]\tLoss: 1.513585\n",
      "Train Epoch: 9 [3000/60000 (5%)]\tLoss: 1.169465\n",
      "Train Epoch: 9 [4000/60000 (7%)]\tLoss: 1.492539\n",
      "Train Epoch: 9 [5000/60000 (8%)]\tLoss: 1.322621\n",
      "Train Epoch: 9 [6000/60000 (10%)]\tLoss: 1.370794\n",
      "Train Epoch: 9 [7000/60000 (12%)]\tLoss: 1.723184\n",
      "Train Epoch: 9 [8000/60000 (13%)]\tLoss: 1.117017\n",
      "Train Epoch: 9 [9000/60000 (15%)]\tLoss: 1.817650\n",
      "Train Epoch: 9 [10000/60000 (17%)]\tLoss: 1.324002\n",
      "Train Epoch: 9 [11000/60000 (18%)]\tLoss: 1.352410\n",
      "Train Epoch: 9 [12000/60000 (20%)]\tLoss: 1.027973\n",
      "Train Epoch: 9 [13000/60000 (22%)]\tLoss: 1.134857\n",
      "Train Epoch: 9 [14000/60000 (23%)]\tLoss: 1.395208\n",
      "Train Epoch: 9 [15000/60000 (25%)]\tLoss: 1.311293\n",
      "Train Epoch: 9 [16000/60000 (27%)]\tLoss: 1.155076\n",
      "Train Epoch: 9 [17000/60000 (28%)]\tLoss: 1.520082\n",
      "Train Epoch: 9 [18000/60000 (30%)]\tLoss: 1.065044\n",
      "Train Epoch: 9 [19000/60000 (32%)]\tLoss: 1.381996\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 1.270922\n",
      "Train Epoch: 9 [21000/60000 (35%)]\tLoss: 1.036244\n",
      "Train Epoch: 9 [22000/60000 (37%)]\tLoss: 0.852714\n",
      "Train Epoch: 9 [23000/60000 (38%)]\tLoss: 1.213268\n",
      "Train Epoch: 9 [24000/60000 (40%)]\tLoss: 1.408337\n",
      "Train Epoch: 9 [25000/60000 (42%)]\tLoss: 1.485078\n",
      "Train Epoch: 9 [26000/60000 (43%)]\tLoss: 1.277537\n",
      "Train Epoch: 9 [27000/60000 (45%)]\tLoss: 1.349445\n",
      "Train Epoch: 9 [28000/60000 (47%)]\tLoss: 1.990673\n",
      "Train Epoch: 9 [29000/60000 (48%)]\tLoss: 1.629763\n",
      "Train Epoch: 9 [30000/60000 (50%)]\tLoss: 0.808197\n",
      "Train Epoch: 9 [31000/60000 (52%)]\tLoss: 0.977678\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 1.543476\n",
      "Train Epoch: 9 [33000/60000 (55%)]\tLoss: 2.463269\n",
      "Train Epoch: 9 [34000/60000 (57%)]\tLoss: 1.607809\n",
      "Train Epoch: 9 [35000/60000 (58%)]\tLoss: 1.041716\n",
      "Train Epoch: 9 [36000/60000 (60%)]\tLoss: 1.455371\n",
      "Train Epoch: 9 [37000/60000 (62%)]\tLoss: 1.524244\n",
      "Train Epoch: 9 [38000/60000 (63%)]\tLoss: 1.256263\n",
      "Train Epoch: 9 [39000/60000 (65%)]\tLoss: 1.275484\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 1.447053\n",
      "Train Epoch: 9 [41000/60000 (68%)]\tLoss: 1.609234\n",
      "Train Epoch: 9 [42000/60000 (70%)]\tLoss: 1.589761\n",
      "Train Epoch: 9 [43000/60000 (72%)]\tLoss: 1.009465\n",
      "Train Epoch: 9 [44000/60000 (73%)]\tLoss: 1.603127\n",
      "Train Epoch: 9 [45000/60000 (75%)]\tLoss: 1.071787\n",
      "Train Epoch: 9 [46000/60000 (77%)]\tLoss: 1.138704\n",
      "Train Epoch: 9 [47000/60000 (78%)]\tLoss: 1.716415\n",
      "Train Epoch: 9 [48000/60000 (80%)]\tLoss: 1.317272\n",
      "Train Epoch: 9 [49000/60000 (82%)]\tLoss: 1.358736\n",
      "Train Epoch: 9 [50000/60000 (83%)]\tLoss: 1.028744\n",
      "Train Epoch: 9 [51000/60000 (85%)]\tLoss: 1.707639\n",
      "Train Epoch: 9 [52000/60000 (87%)]\tLoss: 1.505178\n",
      "Train Epoch: 9 [53000/60000 (88%)]\tLoss: 1.447485\n",
      "Train Epoch: 9 [54000/60000 (90%)]\tLoss: 1.624147\n",
      "Train Epoch: 9 [55000/60000 (92%)]\tLoss: 1.328128\n",
      "Train Epoch: 9 [56000/60000 (93%)]\tLoss: 1.247809\n",
      "Train Epoch: 9 [57000/60000 (95%)]\tLoss: 1.569052\n",
      "Train Epoch: 9 [58000/60000 (97%)]\tLoss: 1.536048\n",
      "Train Epoch: 9 [59000/60000 (98%)]\tLoss: 1.422480\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 1.069844\n",
      "Train Epoch: 10 [1000/60000 (2%)]\tLoss: 1.755980\n",
      "Train Epoch: 10 [2000/60000 (3%)]\tLoss: 1.016735\n",
      "Train Epoch: 10 [3000/60000 (5%)]\tLoss: 1.298019\n",
      "Train Epoch: 10 [4000/60000 (7%)]\tLoss: 1.645047\n",
      "Train Epoch: 10 [5000/60000 (8%)]\tLoss: 1.355921\n",
      "Train Epoch: 10 [6000/60000 (10%)]\tLoss: 0.882250\n",
      "Train Epoch: 10 [7000/60000 (12%)]\tLoss: 1.059152\n",
      "Train Epoch: 10 [8000/60000 (13%)]\tLoss: 1.199022\n",
      "Train Epoch: 10 [9000/60000 (15%)]\tLoss: 1.709626\n",
      "Train Epoch: 10 [10000/60000 (17%)]\tLoss: 0.996373\n",
      "Train Epoch: 10 [11000/60000 (18%)]\tLoss: 1.031517\n",
      "Train Epoch: 10 [12000/60000 (20%)]\tLoss: 1.430200\n",
      "Train Epoch: 10 [13000/60000 (22%)]\tLoss: 0.973792\n",
      "Train Epoch: 10 [14000/60000 (23%)]\tLoss: 1.376609\n",
      "Train Epoch: 10 [15000/60000 (25%)]\tLoss: 1.632562\n",
      "Train Epoch: 10 [16000/60000 (27%)]\tLoss: 1.000420\n",
      "Train Epoch: 10 [17000/60000 (28%)]\tLoss: 1.117247\n",
      "Train Epoch: 10 [18000/60000 (30%)]\tLoss: 1.289031\n",
      "Train Epoch: 10 [19000/60000 (32%)]\tLoss: 1.315451\n",
      "Train Epoch: 10 [20000/60000 (33%)]\tLoss: 1.293322\n",
      "Train Epoch: 10 [21000/60000 (35%)]\tLoss: 1.279672\n",
      "Train Epoch: 10 [22000/60000 (37%)]\tLoss: 1.136264\n",
      "Train Epoch: 10 [23000/60000 (38%)]\tLoss: 1.651932\n",
      "Train Epoch: 10 [24000/60000 (40%)]\tLoss: 1.114238\n",
      "Train Epoch: 10 [25000/60000 (42%)]\tLoss: 1.289973\n",
      "Train Epoch: 10 [26000/60000 (43%)]\tLoss: 1.365907\n",
      "Train Epoch: 10 [27000/60000 (45%)]\tLoss: 1.047620\n",
      "Train Epoch: 10 [28000/60000 (47%)]\tLoss: 1.138144\n",
      "Train Epoch: 10 [29000/60000 (48%)]\tLoss: 1.452783\n",
      "Train Epoch: 10 [30000/60000 (50%)]\tLoss: 1.703487\n",
      "Train Epoch: 10 [31000/60000 (52%)]\tLoss: 1.090901\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 1.074630\n",
      "Train Epoch: 10 [33000/60000 (55%)]\tLoss: 1.211036\n",
      "Train Epoch: 10 [34000/60000 (57%)]\tLoss: 1.358931\n",
      "Train Epoch: 10 [35000/60000 (58%)]\tLoss: 1.806577\n",
      "Train Epoch: 10 [36000/60000 (60%)]\tLoss: 0.899132\n",
      "Train Epoch: 10 [37000/60000 (62%)]\tLoss: 1.286917\n",
      "Train Epoch: 10 [38000/60000 (63%)]\tLoss: 0.840190\n",
      "Train Epoch: 10 [39000/60000 (65%)]\tLoss: 1.067190\n",
      "Train Epoch: 10 [40000/60000 (67%)]\tLoss: 1.102980\n",
      "Train Epoch: 10 [41000/60000 (68%)]\tLoss: 1.582333\n",
      "Train Epoch: 10 [42000/60000 (70%)]\tLoss: 1.361163\n",
      "Train Epoch: 10 [43000/60000 (72%)]\tLoss: 2.002593\n",
      "Train Epoch: 10 [44000/60000 (73%)]\tLoss: 0.970446\n",
      "Train Epoch: 10 [45000/60000 (75%)]\tLoss: 1.317145\n",
      "Train Epoch: 10 [46000/60000 (77%)]\tLoss: 1.160553\n",
      "Train Epoch: 10 [47000/60000 (78%)]\tLoss: 1.198376\n",
      "Train Epoch: 10 [48000/60000 (80%)]\tLoss: 1.250682\n",
      "Train Epoch: 10 [49000/60000 (82%)]\tLoss: 1.093390\n",
      "Train Epoch: 10 [50000/60000 (83%)]\tLoss: 1.095160\n",
      "Train Epoch: 10 [51000/60000 (85%)]\tLoss: 1.228078\n",
      "Train Epoch: 10 [52000/60000 (87%)]\tLoss: 1.545010\n",
      "Train Epoch: 10 [53000/60000 (88%)]\tLoss: 1.563874\n",
      "Train Epoch: 10 [54000/60000 (90%)]\tLoss: 1.178921\n",
      "Train Epoch: 10 [55000/60000 (92%)]\tLoss: 1.483428\n",
      "Train Epoch: 10 [56000/60000 (93%)]\tLoss: 1.149865\n",
      "Train Epoch: 10 [57000/60000 (95%)]\tLoss: 1.149259\n",
      "Train Epoch: 10 [58000/60000 (97%)]\tLoss: 1.351502\n",
      "Train Epoch: 10 [59000/60000 (98%)]\tLoss: 0.955524\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 1.695566\n",
      "Train Epoch: 11 [1000/60000 (2%)]\tLoss: 1.220461\n",
      "Train Epoch: 11 [2000/60000 (3%)]\tLoss: 1.203187\n",
      "Train Epoch: 11 [3000/60000 (5%)]\tLoss: 1.214523\n",
      "Train Epoch: 11 [4000/60000 (7%)]\tLoss: 1.289620\n",
      "Train Epoch: 11 [5000/60000 (8%)]\tLoss: 1.314161\n",
      "Train Epoch: 11 [6000/60000 (10%)]\tLoss: 1.021470\n",
      "Train Epoch: 11 [7000/60000 (12%)]\tLoss: 1.266739\n",
      "Train Epoch: 11 [8000/60000 (13%)]\tLoss: 1.135238\n",
      "Train Epoch: 11 [9000/60000 (15%)]\tLoss: 1.252643\n",
      "Train Epoch: 11 [10000/60000 (17%)]\tLoss: 1.567223\n",
      "Train Epoch: 11 [11000/60000 (18%)]\tLoss: 1.450730\n",
      "Train Epoch: 11 [12000/60000 (20%)]\tLoss: 1.045786\n",
      "Train Epoch: 11 [13000/60000 (22%)]\tLoss: 1.145791\n",
      "Train Epoch: 11 [14000/60000 (23%)]\tLoss: 1.761253\n",
      "Train Epoch: 11 [15000/60000 (25%)]\tLoss: 1.208773\n",
      "Train Epoch: 11 [16000/60000 (27%)]\tLoss: 1.232235\n",
      "Train Epoch: 11 [17000/60000 (28%)]\tLoss: 1.077483\n",
      "Train Epoch: 11 [18000/60000 (30%)]\tLoss: 1.085636\n",
      "Train Epoch: 11 [19000/60000 (32%)]\tLoss: 1.365077\n",
      "Train Epoch: 11 [20000/60000 (33%)]\tLoss: 0.742939\n",
      "Train Epoch: 11 [21000/60000 (35%)]\tLoss: 1.620018\n",
      "Train Epoch: 11 [22000/60000 (37%)]\tLoss: 1.068305\n",
      "Train Epoch: 11 [23000/60000 (38%)]\tLoss: 1.101950\n",
      "Train Epoch: 11 [24000/60000 (40%)]\tLoss: 1.017530\n",
      "Train Epoch: 11 [25000/60000 (42%)]\tLoss: 1.293917\n",
      "Train Epoch: 11 [26000/60000 (43%)]\tLoss: 1.037307\n",
      "Train Epoch: 11 [27000/60000 (45%)]\tLoss: 1.480772\n",
      "Train Epoch: 11 [28000/60000 (47%)]\tLoss: 1.282017\n",
      "Train Epoch: 11 [29000/60000 (48%)]\tLoss: 0.954630\n",
      "Train Epoch: 11 [30000/60000 (50%)]\tLoss: 1.223768\n",
      "Train Epoch: 11 [31000/60000 (52%)]\tLoss: 1.078943\n",
      "Train Epoch: 11 [32000/60000 (53%)]\tLoss: 1.082075\n",
      "Train Epoch: 11 [33000/60000 (55%)]\tLoss: 1.278323\n",
      "Train Epoch: 11 [34000/60000 (57%)]\tLoss: 1.253060\n",
      "Train Epoch: 11 [35000/60000 (58%)]\tLoss: 1.548924\n",
      "Train Epoch: 11 [36000/60000 (60%)]\tLoss: 0.892373\n",
      "Train Epoch: 11 [37000/60000 (62%)]\tLoss: 1.133353\n",
      "Train Epoch: 11 [38000/60000 (63%)]\tLoss: 1.475225\n",
      "Train Epoch: 11 [39000/60000 (65%)]\tLoss: 1.368062\n",
      "Train Epoch: 11 [40000/60000 (67%)]\tLoss: 1.013495\n",
      "Train Epoch: 11 [41000/60000 (68%)]\tLoss: 1.376543\n",
      "Train Epoch: 11 [42000/60000 (70%)]\tLoss: 1.010198\n",
      "Train Epoch: 11 [43000/60000 (72%)]\tLoss: 1.126384\n",
      "Train Epoch: 11 [44000/60000 (73%)]\tLoss: 1.413074\n",
      "Train Epoch: 11 [45000/60000 (75%)]\tLoss: 1.650808\n",
      "Train Epoch: 11 [46000/60000 (77%)]\tLoss: 1.021713\n",
      "Train Epoch: 11 [47000/60000 (78%)]\tLoss: 1.265265\n",
      "Train Epoch: 11 [48000/60000 (80%)]\tLoss: 1.307383\n",
      "Train Epoch: 11 [49000/60000 (82%)]\tLoss: 1.378106\n",
      "Train Epoch: 11 [50000/60000 (83%)]\tLoss: 1.184237\n",
      "Train Epoch: 11 [51000/60000 (85%)]\tLoss: 1.077046\n",
      "Train Epoch: 11 [52000/60000 (87%)]\tLoss: 1.198133\n",
      "Train Epoch: 11 [53000/60000 (88%)]\tLoss: 1.421969\n",
      "Train Epoch: 11 [54000/60000 (90%)]\tLoss: 1.302889\n",
      "Train Epoch: 11 [55000/60000 (92%)]\tLoss: 1.085566\n",
      "Train Epoch: 11 [56000/60000 (93%)]\tLoss: 1.013653\n",
      "Train Epoch: 11 [57000/60000 (95%)]\tLoss: 1.267298\n",
      "Train Epoch: 11 [58000/60000 (97%)]\tLoss: 1.255608\n",
      "Train Epoch: 11 [59000/60000 (98%)]\tLoss: 0.945611\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 1.499029\n",
      "Train Epoch: 12 [1000/60000 (2%)]\tLoss: 0.835615\n",
      "Train Epoch: 12 [2000/60000 (3%)]\tLoss: 1.035165\n",
      "Train Epoch: 12 [3000/60000 (5%)]\tLoss: 0.961836\n",
      "Train Epoch: 12 [4000/60000 (7%)]\tLoss: 1.081214\n",
      "Train Epoch: 12 [5000/60000 (8%)]\tLoss: 1.097949\n",
      "Train Epoch: 12 [6000/60000 (10%)]\tLoss: 1.189826\n",
      "Train Epoch: 12 [7000/60000 (12%)]\tLoss: 1.008052\n",
      "Train Epoch: 12 [8000/60000 (13%)]\tLoss: 1.762286\n",
      "Train Epoch: 12 [9000/60000 (15%)]\tLoss: 1.135501\n",
      "Train Epoch: 12 [10000/60000 (17%)]\tLoss: 1.118598\n",
      "Train Epoch: 12 [11000/60000 (18%)]\tLoss: 1.325156\n",
      "Train Epoch: 12 [12000/60000 (20%)]\tLoss: 1.741803\n",
      "Train Epoch: 12 [13000/60000 (22%)]\tLoss: 1.424301\n",
      "Train Epoch: 12 [14000/60000 (23%)]\tLoss: 1.231836\n",
      "Train Epoch: 12 [15000/60000 (25%)]\tLoss: 1.385703\n",
      "Train Epoch: 12 [16000/60000 (27%)]\tLoss: 1.043274\n",
      "Train Epoch: 12 [17000/60000 (28%)]\tLoss: 1.512015\n",
      "Train Epoch: 12 [18000/60000 (30%)]\tLoss: 0.902731\n",
      "Train Epoch: 12 [19000/60000 (32%)]\tLoss: 1.085743\n",
      "Train Epoch: 12 [20000/60000 (33%)]\tLoss: 1.302158\n",
      "Train Epoch: 12 [21000/60000 (35%)]\tLoss: 0.823977\n",
      "Train Epoch: 12 [22000/60000 (37%)]\tLoss: 1.186850\n",
      "Train Epoch: 12 [23000/60000 (38%)]\tLoss: 1.482805\n",
      "Train Epoch: 12 [24000/60000 (40%)]\tLoss: 1.272075\n",
      "Train Epoch: 12 [25000/60000 (42%)]\tLoss: 1.317526\n",
      "Train Epoch: 12 [26000/60000 (43%)]\tLoss: 1.284268\n",
      "Train Epoch: 12 [27000/60000 (45%)]\tLoss: 1.203291\n",
      "Train Epoch: 12 [28000/60000 (47%)]\tLoss: 1.033041\n",
      "Train Epoch: 12 [29000/60000 (48%)]\tLoss: 1.047112\n",
      "Train Epoch: 12 [30000/60000 (50%)]\tLoss: 1.280616\n",
      "Train Epoch: 12 [31000/60000 (52%)]\tLoss: 1.033408\n",
      "Train Epoch: 12 [32000/60000 (53%)]\tLoss: 1.611749\n",
      "Train Epoch: 12 [33000/60000 (55%)]\tLoss: 0.922979\n",
      "Train Epoch: 12 [34000/60000 (57%)]\tLoss: 1.255352\n",
      "Train Epoch: 12 [35000/60000 (58%)]\tLoss: 1.178990\n",
      "Train Epoch: 12 [36000/60000 (60%)]\tLoss: 1.467461\n",
      "Train Epoch: 12 [37000/60000 (62%)]\tLoss: 0.878003\n",
      "Train Epoch: 12 [38000/60000 (63%)]\tLoss: 0.864320\n",
      "Train Epoch: 12 [39000/60000 (65%)]\tLoss: 1.724348\n",
      "Train Epoch: 12 [40000/60000 (67%)]\tLoss: 1.382110\n",
      "Train Epoch: 12 [41000/60000 (68%)]\tLoss: 1.445757\n",
      "Train Epoch: 12 [42000/60000 (70%)]\tLoss: 1.323702\n",
      "Train Epoch: 12 [43000/60000 (72%)]\tLoss: 0.985047\n",
      "Train Epoch: 12 [44000/60000 (73%)]\tLoss: 1.046648\n",
      "Train Epoch: 12 [45000/60000 (75%)]\tLoss: 1.123949\n",
      "Train Epoch: 12 [46000/60000 (77%)]\tLoss: 1.288091\n",
      "Train Epoch: 12 [47000/60000 (78%)]\tLoss: 0.862269\n",
      "Train Epoch: 12 [48000/60000 (80%)]\tLoss: 1.149085\n",
      "Train Epoch: 12 [49000/60000 (82%)]\tLoss: 0.883768\n",
      "Train Epoch: 12 [50000/60000 (83%)]\tLoss: 1.411050\n",
      "Train Epoch: 12 [51000/60000 (85%)]\tLoss: 0.937290\n",
      "Train Epoch: 12 [52000/60000 (87%)]\tLoss: 0.997548\n",
      "Train Epoch: 12 [53000/60000 (88%)]\tLoss: 1.185691\n",
      "Train Epoch: 12 [54000/60000 (90%)]\tLoss: 1.309250\n",
      "Train Epoch: 12 [55000/60000 (92%)]\tLoss: 0.890523\n",
      "Train Epoch: 12 [56000/60000 (93%)]\tLoss: 1.400439\n",
      "Train Epoch: 12 [57000/60000 (95%)]\tLoss: 1.426800\n",
      "Train Epoch: 12 [58000/60000 (97%)]\tLoss: 0.982767\n",
      "Train Epoch: 12 [59000/60000 (98%)]\tLoss: 1.003151\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 1.450994\n",
      "Train Epoch: 13 [1000/60000 (2%)]\tLoss: 1.238139\n",
      "Train Epoch: 13 [2000/60000 (3%)]\tLoss: 0.995708\n",
      "Train Epoch: 13 [3000/60000 (5%)]\tLoss: 1.288996\n",
      "Train Epoch: 13 [4000/60000 (7%)]\tLoss: 1.424202\n",
      "Train Epoch: 13 [5000/60000 (8%)]\tLoss: 1.210450\n",
      "Train Epoch: 13 [6000/60000 (10%)]\tLoss: 1.340862\n",
      "Train Epoch: 13 [7000/60000 (12%)]\tLoss: 1.092334\n",
      "Train Epoch: 13 [8000/60000 (13%)]\tLoss: 1.604693\n",
      "Train Epoch: 13 [9000/60000 (15%)]\tLoss: 1.531768\n",
      "Train Epoch: 13 [10000/60000 (17%)]\tLoss: 1.491636\n",
      "Train Epoch: 13 [11000/60000 (18%)]\tLoss: 1.149911\n",
      "Train Epoch: 13 [12000/60000 (20%)]\tLoss: 0.988078\n",
      "Train Epoch: 13 [13000/60000 (22%)]\tLoss: 1.076038\n",
      "Train Epoch: 13 [14000/60000 (23%)]\tLoss: 1.169734\n",
      "Train Epoch: 13 [15000/60000 (25%)]\tLoss: 1.147921\n",
      "Train Epoch: 13 [16000/60000 (27%)]\tLoss: 1.520801\n",
      "Train Epoch: 13 [17000/60000 (28%)]\tLoss: 1.081825\n",
      "Train Epoch: 13 [18000/60000 (30%)]\tLoss: 1.157838\n",
      "Train Epoch: 13 [19000/60000 (32%)]\tLoss: 1.217596\n",
      "Train Epoch: 13 [20000/60000 (33%)]\tLoss: 0.990201\n",
      "Train Epoch: 13 [21000/60000 (35%)]\tLoss: 1.338757\n",
      "Train Epoch: 13 [22000/60000 (37%)]\tLoss: 1.330446\n",
      "Train Epoch: 13 [23000/60000 (38%)]\tLoss: 1.131225\n",
      "Train Epoch: 13 [24000/60000 (40%)]\tLoss: 1.017891\n",
      "Train Epoch: 13 [25000/60000 (42%)]\tLoss: 1.587204\n",
      "Train Epoch: 13 [26000/60000 (43%)]\tLoss: 1.446571\n",
      "Train Epoch: 13 [27000/60000 (45%)]\tLoss: 1.241593\n",
      "Train Epoch: 13 [28000/60000 (47%)]\tLoss: 1.698649\n",
      "Train Epoch: 13 [29000/60000 (48%)]\tLoss: 0.687445\n",
      "Train Epoch: 13 [30000/60000 (50%)]\tLoss: 1.311693\n",
      "Train Epoch: 13 [31000/60000 (52%)]\tLoss: 1.421808\n",
      "Train Epoch: 13 [32000/60000 (53%)]\tLoss: 1.200896\n",
      "Train Epoch: 13 [33000/60000 (55%)]\tLoss: 1.178649\n",
      "Train Epoch: 13 [34000/60000 (57%)]\tLoss: 1.402019\n",
      "Train Epoch: 13 [35000/60000 (58%)]\tLoss: 1.315073\n",
      "Train Epoch: 13 [36000/60000 (60%)]\tLoss: 1.417622\n",
      "Train Epoch: 13 [37000/60000 (62%)]\tLoss: 1.332695\n",
      "Train Epoch: 13 [38000/60000 (63%)]\tLoss: 1.368462\n",
      "Train Epoch: 13 [39000/60000 (65%)]\tLoss: 1.666034\n",
      "Train Epoch: 13 [40000/60000 (67%)]\tLoss: 1.417522\n",
      "Train Epoch: 13 [41000/60000 (68%)]\tLoss: 1.229525\n",
      "Train Epoch: 13 [42000/60000 (70%)]\tLoss: 0.883652\n",
      "Train Epoch: 13 [43000/60000 (72%)]\tLoss: 1.476225\n",
      "Train Epoch: 13 [44000/60000 (73%)]\tLoss: 1.098475\n",
      "Train Epoch: 13 [45000/60000 (75%)]\tLoss: 1.600780\n",
      "Train Epoch: 13 [46000/60000 (77%)]\tLoss: 1.074738\n",
      "Train Epoch: 13 [47000/60000 (78%)]\tLoss: 1.206056\n",
      "Train Epoch: 13 [48000/60000 (80%)]\tLoss: 1.147912\n",
      "Train Epoch: 13 [49000/60000 (82%)]\tLoss: 0.602012\n",
      "Train Epoch: 13 [50000/60000 (83%)]\tLoss: 1.146470\n",
      "Train Epoch: 13 [51000/60000 (85%)]\tLoss: 1.345938\n",
      "Train Epoch: 13 [52000/60000 (87%)]\tLoss: 1.289015\n",
      "Train Epoch: 13 [53000/60000 (88%)]\tLoss: 1.145428\n",
      "Train Epoch: 13 [54000/60000 (90%)]\tLoss: 1.199403\n",
      "Train Epoch: 13 [55000/60000 (92%)]\tLoss: 0.842924\n",
      "Train Epoch: 13 [56000/60000 (93%)]\tLoss: 0.885068\n",
      "Train Epoch: 13 [57000/60000 (95%)]\tLoss: 1.337409\n",
      "Train Epoch: 13 [58000/60000 (97%)]\tLoss: 1.712712\n",
      "Train Epoch: 13 [59000/60000 (98%)]\tLoss: 0.901450\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 1.061773\n",
      "Train Epoch: 14 [1000/60000 (2%)]\tLoss: 0.967627\n",
      "Train Epoch: 14 [2000/60000 (3%)]\tLoss: 1.124911\n",
      "Train Epoch: 14 [3000/60000 (5%)]\tLoss: 1.162655\n",
      "Train Epoch: 14 [4000/60000 (7%)]\tLoss: 0.972166\n",
      "Train Epoch: 14 [5000/60000 (8%)]\tLoss: 0.850159\n",
      "Train Epoch: 14 [6000/60000 (10%)]\tLoss: 1.527984\n",
      "Train Epoch: 14 [7000/60000 (12%)]\tLoss: 1.292126\n",
      "Train Epoch: 14 [8000/60000 (13%)]\tLoss: 1.029105\n",
      "Train Epoch: 14 [9000/60000 (15%)]\tLoss: 1.021373\n",
      "Train Epoch: 14 [10000/60000 (17%)]\tLoss: 1.399734\n",
      "Train Epoch: 14 [11000/60000 (18%)]\tLoss: 1.145115\n",
      "Train Epoch: 14 [12000/60000 (20%)]\tLoss: 1.275482\n",
      "Train Epoch: 14 [13000/60000 (22%)]\tLoss: 0.806013\n",
      "Train Epoch: 14 [14000/60000 (23%)]\tLoss: 1.135632\n",
      "Train Epoch: 14 [15000/60000 (25%)]\tLoss: 1.530020\n",
      "Train Epoch: 14 [16000/60000 (27%)]\tLoss: 0.950378\n",
      "Train Epoch: 14 [17000/60000 (28%)]\tLoss: 1.013503\n",
      "Train Epoch: 14 [18000/60000 (30%)]\tLoss: 1.107512\n",
      "Train Epoch: 14 [19000/60000 (32%)]\tLoss: 1.204593\n",
      "Train Epoch: 14 [20000/60000 (33%)]\tLoss: 1.845729\n",
      "Train Epoch: 14 [21000/60000 (35%)]\tLoss: 0.719194\n",
      "Train Epoch: 14 [22000/60000 (37%)]\tLoss: 1.052571\n",
      "Train Epoch: 14 [23000/60000 (38%)]\tLoss: 1.182783\n",
      "Train Epoch: 14 [24000/60000 (40%)]\tLoss: 1.048128\n",
      "Train Epoch: 14 [25000/60000 (42%)]\tLoss: 1.222656\n",
      "Train Epoch: 14 [26000/60000 (43%)]\tLoss: 1.308598\n",
      "Train Epoch: 14 [27000/60000 (45%)]\tLoss: 0.964970\n",
      "Train Epoch: 14 [28000/60000 (47%)]\tLoss: 1.526714\n",
      "Train Epoch: 14 [29000/60000 (48%)]\tLoss: 1.532350\n",
      "Train Epoch: 14 [30000/60000 (50%)]\tLoss: 0.931309\n",
      "Train Epoch: 14 [31000/60000 (52%)]\tLoss: 1.108137\n",
      "Train Epoch: 14 [32000/60000 (53%)]\tLoss: 1.300098\n",
      "Train Epoch: 14 [33000/60000 (55%)]\tLoss: 1.083994\n",
      "Train Epoch: 14 [34000/60000 (57%)]\tLoss: 1.568828\n",
      "Train Epoch: 14 [35000/60000 (58%)]\tLoss: 1.556993\n",
      "Train Epoch: 14 [36000/60000 (60%)]\tLoss: 1.248783\n",
      "Train Epoch: 14 [37000/60000 (62%)]\tLoss: 1.224011\n",
      "Train Epoch: 14 [38000/60000 (63%)]\tLoss: 1.457478\n",
      "Train Epoch: 14 [39000/60000 (65%)]\tLoss: 0.894238\n",
      "Train Epoch: 14 [40000/60000 (67%)]\tLoss: 1.622164\n",
      "Train Epoch: 14 [41000/60000 (68%)]\tLoss: 1.224646\n",
      "Train Epoch: 14 [42000/60000 (70%)]\tLoss: 1.232251\n",
      "Train Epoch: 14 [43000/60000 (72%)]\tLoss: 1.435964\n",
      "Train Epoch: 14 [44000/60000 (73%)]\tLoss: 1.188881\n",
      "Train Epoch: 14 [45000/60000 (75%)]\tLoss: 1.151106\n",
      "Train Epoch: 14 [46000/60000 (77%)]\tLoss: 0.988546\n",
      "Train Epoch: 14 [47000/60000 (78%)]\tLoss: 1.383245\n",
      "Train Epoch: 14 [48000/60000 (80%)]\tLoss: 1.019641\n",
      "Train Epoch: 14 [49000/60000 (82%)]\tLoss: 0.810988\n",
      "Train Epoch: 14 [50000/60000 (83%)]\tLoss: 1.257184\n",
      "Train Epoch: 14 [51000/60000 (85%)]\tLoss: 1.068156\n",
      "Train Epoch: 14 [52000/60000 (87%)]\tLoss: 1.223024\n",
      "Train Epoch: 14 [53000/60000 (88%)]\tLoss: 0.732440\n",
      "Train Epoch: 14 [54000/60000 (90%)]\tLoss: 0.984495\n",
      "Train Epoch: 14 [55000/60000 (92%)]\tLoss: 1.475667\n",
      "Train Epoch: 14 [56000/60000 (93%)]\tLoss: 1.313852\n",
      "Train Epoch: 14 [57000/60000 (95%)]\tLoss: 1.349903\n",
      "Train Epoch: 14 [58000/60000 (97%)]\tLoss: 1.035495\n",
      "Train Epoch: 14 [59000/60000 (98%)]\tLoss: 0.972607\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 1.317685\n",
      "Train Epoch: 15 [1000/60000 (2%)]\tLoss: 1.223907\n",
      "Train Epoch: 15 [2000/60000 (3%)]\tLoss: 1.028366\n",
      "Train Epoch: 15 [3000/60000 (5%)]\tLoss: 0.887493\n",
      "Train Epoch: 15 [4000/60000 (7%)]\tLoss: 0.998972\n",
      "Train Epoch: 15 [5000/60000 (8%)]\tLoss: 1.303552\n",
      "Train Epoch: 15 [6000/60000 (10%)]\tLoss: 1.021939\n",
      "Train Epoch: 15 [7000/60000 (12%)]\tLoss: 1.101148\n",
      "Train Epoch: 15 [8000/60000 (13%)]\tLoss: 1.305439\n",
      "Train Epoch: 15 [9000/60000 (15%)]\tLoss: 1.092081\n",
      "Train Epoch: 15 [10000/60000 (17%)]\tLoss: 1.043042\n",
      "Train Epoch: 15 [11000/60000 (18%)]\tLoss: 1.809940\n",
      "Train Epoch: 15 [12000/60000 (20%)]\tLoss: 0.882626\n",
      "Train Epoch: 15 [13000/60000 (22%)]\tLoss: 1.569344\n",
      "Train Epoch: 15 [14000/60000 (23%)]\tLoss: 1.063852\n",
      "Train Epoch: 15 [15000/60000 (25%)]\tLoss: 1.435787\n",
      "Train Epoch: 15 [16000/60000 (27%)]\tLoss: 1.001375\n",
      "Train Epoch: 15 [17000/60000 (28%)]\tLoss: 1.016802\n",
      "Train Epoch: 15 [18000/60000 (30%)]\tLoss: 1.166397\n",
      "Train Epoch: 15 [19000/60000 (32%)]\tLoss: 0.931066\n",
      "Train Epoch: 15 [20000/60000 (33%)]\tLoss: 1.577981\n",
      "Train Epoch: 15 [21000/60000 (35%)]\tLoss: 0.922130\n",
      "Train Epoch: 15 [22000/60000 (37%)]\tLoss: 0.939013\n",
      "Train Epoch: 15 [23000/60000 (38%)]\tLoss: 0.828682\n",
      "Train Epoch: 15 [24000/60000 (40%)]\tLoss: 1.282756\n",
      "Train Epoch: 15 [25000/60000 (42%)]\tLoss: 0.973168\n",
      "Train Epoch: 15 [26000/60000 (43%)]\tLoss: 1.075003\n",
      "Train Epoch: 15 [27000/60000 (45%)]\tLoss: 1.346731\n",
      "Train Epoch: 15 [28000/60000 (47%)]\tLoss: 1.236227\n",
      "Train Epoch: 15 [29000/60000 (48%)]\tLoss: 1.325952\n",
      "Train Epoch: 15 [30000/60000 (50%)]\tLoss: 1.223610\n",
      "Train Epoch: 15 [31000/60000 (52%)]\tLoss: 1.432396\n",
      "Train Epoch: 15 [32000/60000 (53%)]\tLoss: 1.523571\n",
      "Train Epoch: 15 [33000/60000 (55%)]\tLoss: 0.983144\n",
      "Train Epoch: 15 [34000/60000 (57%)]\tLoss: 0.837738\n",
      "Train Epoch: 15 [35000/60000 (58%)]\tLoss: 1.164098\n",
      "Train Epoch: 15 [36000/60000 (60%)]\tLoss: 1.076995\n",
      "Train Epoch: 15 [37000/60000 (62%)]\tLoss: 0.770764\n",
      "Train Epoch: 15 [38000/60000 (63%)]\tLoss: 0.992956\n",
      "Train Epoch: 15 [39000/60000 (65%)]\tLoss: 1.416981\n",
      "Train Epoch: 15 [40000/60000 (67%)]\tLoss: 0.896672\n",
      "Train Epoch: 15 [41000/60000 (68%)]\tLoss: 1.317523\n",
      "Train Epoch: 15 [42000/60000 (70%)]\tLoss: 1.005242\n",
      "Train Epoch: 15 [43000/60000 (72%)]\tLoss: 1.004760\n",
      "Train Epoch: 15 [44000/60000 (73%)]\tLoss: 1.014329\n",
      "Train Epoch: 15 [45000/60000 (75%)]\tLoss: 1.296417\n",
      "Train Epoch: 15 [46000/60000 (77%)]\tLoss: 1.640450\n",
      "Train Epoch: 15 [47000/60000 (78%)]\tLoss: 0.852187\n",
      "Train Epoch: 15 [48000/60000 (80%)]\tLoss: 1.275102\n",
      "Train Epoch: 15 [49000/60000 (82%)]\tLoss: 1.179560\n",
      "Train Epoch: 15 [50000/60000 (83%)]\tLoss: 1.224212\n",
      "Train Epoch: 15 [51000/60000 (85%)]\tLoss: 0.989323\n",
      "Train Epoch: 15 [52000/60000 (87%)]\tLoss: 1.245430\n",
      "Train Epoch: 15 [53000/60000 (88%)]\tLoss: 1.446928\n",
      "Train Epoch: 15 [54000/60000 (90%)]\tLoss: 1.328221\n",
      "Train Epoch: 15 [55000/60000 (92%)]\tLoss: 1.036628\n",
      "Train Epoch: 15 [56000/60000 (93%)]\tLoss: 1.293365\n",
      "Train Epoch: 15 [57000/60000 (95%)]\tLoss: 1.051030\n",
      "Train Epoch: 15 [58000/60000 (97%)]\tLoss: 0.960926\n",
      "Train Epoch: 15 [59000/60000 (98%)]\tLoss: 1.002675\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 1.088695\n",
      "Train Epoch: 16 [1000/60000 (2%)]\tLoss: 0.927602\n",
      "Train Epoch: 16 [2000/60000 (3%)]\tLoss: 1.529788\n",
      "Train Epoch: 16 [3000/60000 (5%)]\tLoss: 1.071381\n",
      "Train Epoch: 16 [4000/60000 (7%)]\tLoss: 1.307816\n",
      "Train Epoch: 16 [5000/60000 (8%)]\tLoss: 1.149491\n",
      "Train Epoch: 16 [6000/60000 (10%)]\tLoss: 1.570304\n",
      "Train Epoch: 16 [7000/60000 (12%)]\tLoss: 1.113000\n",
      "Train Epoch: 16 [8000/60000 (13%)]\tLoss: 1.010874\n",
      "Train Epoch: 16 [9000/60000 (15%)]\tLoss: 1.173088\n",
      "Train Epoch: 16 [10000/60000 (17%)]\tLoss: 0.672603\n",
      "Train Epoch: 16 [11000/60000 (18%)]\tLoss: 1.270977\n",
      "Train Epoch: 16 [12000/60000 (20%)]\tLoss: 0.930350\n",
      "Train Epoch: 16 [13000/60000 (22%)]\tLoss: 0.954330\n",
      "Train Epoch: 16 [14000/60000 (23%)]\tLoss: 0.865492\n",
      "Train Epoch: 16 [15000/60000 (25%)]\tLoss: 1.093672\n",
      "Train Epoch: 16 [16000/60000 (27%)]\tLoss: 1.017109\n",
      "Train Epoch: 16 [17000/60000 (28%)]\tLoss: 0.843531\n",
      "Train Epoch: 16 [18000/60000 (30%)]\tLoss: 0.940784\n",
      "Train Epoch: 16 [19000/60000 (32%)]\tLoss: 0.953698\n",
      "Train Epoch: 16 [20000/60000 (33%)]\tLoss: 1.326195\n",
      "Train Epoch: 16 [21000/60000 (35%)]\tLoss: 1.300535\n",
      "Train Epoch: 16 [22000/60000 (37%)]\tLoss: 0.990527\n",
      "Train Epoch: 16 [23000/60000 (38%)]\tLoss: 1.379031\n",
      "Train Epoch: 16 [24000/60000 (40%)]\tLoss: 0.804606\n",
      "Train Epoch: 16 [25000/60000 (42%)]\tLoss: 1.153368\n",
      "Train Epoch: 16 [26000/60000 (43%)]\tLoss: 1.155378\n",
      "Train Epoch: 16 [27000/60000 (45%)]\tLoss: 0.764694\n",
      "Train Epoch: 16 [28000/60000 (47%)]\tLoss: 1.342646\n",
      "Train Epoch: 16 [29000/60000 (48%)]\tLoss: 1.029307\n",
      "Train Epoch: 16 [30000/60000 (50%)]\tLoss: 1.220866\n",
      "Train Epoch: 16 [31000/60000 (52%)]\tLoss: 0.927865\n",
      "Train Epoch: 16 [32000/60000 (53%)]\tLoss: 1.448210\n",
      "Train Epoch: 16 [33000/60000 (55%)]\tLoss: 1.117057\n",
      "Train Epoch: 16 [34000/60000 (57%)]\tLoss: 1.352886\n",
      "Train Epoch: 16 [35000/60000 (58%)]\tLoss: 1.300163\n",
      "Train Epoch: 16 [36000/60000 (60%)]\tLoss: 0.859130\n",
      "Train Epoch: 16 [37000/60000 (62%)]\tLoss: 0.969998\n",
      "Train Epoch: 16 [38000/60000 (63%)]\tLoss: 1.170187\n",
      "Train Epoch: 16 [39000/60000 (65%)]\tLoss: 0.832628\n",
      "Train Epoch: 16 [40000/60000 (67%)]\tLoss: 0.968901\n",
      "Train Epoch: 16 [41000/60000 (68%)]\tLoss: 1.234837\n",
      "Train Epoch: 16 [42000/60000 (70%)]\tLoss: 0.905308\n",
      "Train Epoch: 16 [43000/60000 (72%)]\tLoss: 1.186610\n",
      "Train Epoch: 16 [44000/60000 (73%)]\tLoss: 1.124786\n",
      "Train Epoch: 16 [45000/60000 (75%)]\tLoss: 1.268766\n",
      "Train Epoch: 16 [46000/60000 (77%)]\tLoss: 1.085603\n",
      "Train Epoch: 16 [47000/60000 (78%)]\tLoss: 1.064723\n",
      "Train Epoch: 16 [48000/60000 (80%)]\tLoss: 1.509826\n",
      "Train Epoch: 16 [49000/60000 (82%)]\tLoss: 1.085619\n",
      "Train Epoch: 16 [50000/60000 (83%)]\tLoss: 1.422045\n",
      "Train Epoch: 16 [51000/60000 (85%)]\tLoss: 1.447211\n",
      "Train Epoch: 16 [52000/60000 (87%)]\tLoss: 0.932381\n",
      "Train Epoch: 16 [53000/60000 (88%)]\tLoss: 1.341518\n",
      "Train Epoch: 16 [54000/60000 (90%)]\tLoss: 0.917311\n",
      "Train Epoch: 16 [55000/60000 (92%)]\tLoss: 1.617773\n",
      "Train Epoch: 16 [56000/60000 (93%)]\tLoss: 1.264769\n",
      "Train Epoch: 16 [57000/60000 (95%)]\tLoss: 1.282874\n",
      "Train Epoch: 16 [58000/60000 (97%)]\tLoss: 0.815285\n",
      "Train Epoch: 16 [59000/60000 (98%)]\tLoss: 1.122071\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 1.385437\n",
      "Train Epoch: 17 [1000/60000 (2%)]\tLoss: 0.900358\n",
      "Train Epoch: 17 [2000/60000 (3%)]\tLoss: 1.237435\n",
      "Train Epoch: 17 [3000/60000 (5%)]\tLoss: 0.985929\n",
      "Train Epoch: 17 [4000/60000 (7%)]\tLoss: 1.391784\n",
      "Train Epoch: 17 [5000/60000 (8%)]\tLoss: 1.199380\n",
      "Train Epoch: 17 [6000/60000 (10%)]\tLoss: 1.045725\n",
      "Train Epoch: 17 [7000/60000 (12%)]\tLoss: 1.024998\n",
      "Train Epoch: 17 [8000/60000 (13%)]\tLoss: 1.241616\n",
      "Train Epoch: 17 [9000/60000 (15%)]\tLoss: 1.005530\n",
      "Train Epoch: 17 [10000/60000 (17%)]\tLoss: 1.032494\n",
      "Train Epoch: 17 [11000/60000 (18%)]\tLoss: 1.677112\n",
      "Train Epoch: 17 [12000/60000 (20%)]\tLoss: 1.137761\n",
      "Train Epoch: 17 [13000/60000 (22%)]\tLoss: 0.957348\n",
      "Train Epoch: 17 [14000/60000 (23%)]\tLoss: 0.849375\n",
      "Train Epoch: 17 [15000/60000 (25%)]\tLoss: 1.342537\n",
      "Train Epoch: 17 [16000/60000 (27%)]\tLoss: 0.919250\n",
      "Train Epoch: 17 [17000/60000 (28%)]\tLoss: 1.762951\n",
      "Train Epoch: 17 [18000/60000 (30%)]\tLoss: 1.069114\n",
      "Train Epoch: 17 [19000/60000 (32%)]\tLoss: 0.966830\n",
      "Train Epoch: 17 [20000/60000 (33%)]\tLoss: 1.388209\n",
      "Train Epoch: 17 [21000/60000 (35%)]\tLoss: 1.326983\n",
      "Train Epoch: 17 [22000/60000 (37%)]\tLoss: 1.528801\n",
      "Train Epoch: 17 [23000/60000 (38%)]\tLoss: 0.877084\n",
      "Train Epoch: 17 [24000/60000 (40%)]\tLoss: 0.856153\n",
      "Train Epoch: 17 [25000/60000 (42%)]\tLoss: 1.053164\n",
      "Train Epoch: 17 [26000/60000 (43%)]\tLoss: 1.111705\n",
      "Train Epoch: 17 [27000/60000 (45%)]\tLoss: 1.084399\n",
      "Train Epoch: 17 [28000/60000 (47%)]\tLoss: 0.892900\n",
      "Train Epoch: 17 [29000/60000 (48%)]\tLoss: 1.467259\n",
      "Train Epoch: 17 [30000/60000 (50%)]\tLoss: 2.159223\n",
      "Train Epoch: 17 [31000/60000 (52%)]\tLoss: 0.932450\n",
      "Train Epoch: 17 [32000/60000 (53%)]\tLoss: 0.828637\n",
      "Train Epoch: 17 [33000/60000 (55%)]\tLoss: 1.228323\n",
      "Train Epoch: 17 [34000/60000 (57%)]\tLoss: 0.967055\n",
      "Train Epoch: 17 [35000/60000 (58%)]\tLoss: 1.055495\n",
      "Train Epoch: 17 [36000/60000 (60%)]\tLoss: 1.345189\n",
      "Train Epoch: 17 [37000/60000 (62%)]\tLoss: 1.086430\n",
      "Train Epoch: 17 [38000/60000 (63%)]\tLoss: 1.123801\n",
      "Train Epoch: 17 [39000/60000 (65%)]\tLoss: 0.836604\n",
      "Train Epoch: 17 [40000/60000 (67%)]\tLoss: 1.251045\n",
      "Train Epoch: 17 [41000/60000 (68%)]\tLoss: 1.568827\n",
      "Train Epoch: 17 [42000/60000 (70%)]\tLoss: 0.885989\n",
      "Train Epoch: 17 [43000/60000 (72%)]\tLoss: 0.789793\n",
      "Train Epoch: 17 [44000/60000 (73%)]\tLoss: 0.742347\n",
      "Train Epoch: 17 [45000/60000 (75%)]\tLoss: 0.834591\n",
      "Train Epoch: 17 [46000/60000 (77%)]\tLoss: 1.180797\n",
      "Train Epoch: 17 [47000/60000 (78%)]\tLoss: 1.302456\n",
      "Train Epoch: 17 [48000/60000 (80%)]\tLoss: 1.160564\n",
      "Train Epoch: 17 [49000/60000 (82%)]\tLoss: 1.357674\n",
      "Train Epoch: 17 [50000/60000 (83%)]\tLoss: 1.201762\n",
      "Train Epoch: 17 [51000/60000 (85%)]\tLoss: 0.908358\n",
      "Train Epoch: 17 [52000/60000 (87%)]\tLoss: 1.066011\n",
      "Train Epoch: 17 [53000/60000 (88%)]\tLoss: 1.212853\n",
      "Train Epoch: 17 [54000/60000 (90%)]\tLoss: 1.114283\n",
      "Train Epoch: 17 [55000/60000 (92%)]\tLoss: 1.069213\n",
      "Train Epoch: 17 [56000/60000 (93%)]\tLoss: 0.992254\n",
      "Train Epoch: 17 [57000/60000 (95%)]\tLoss: 0.997510\n",
      "Train Epoch: 17 [58000/60000 (97%)]\tLoss: 1.346181\n",
      "Train Epoch: 17 [59000/60000 (98%)]\tLoss: 1.361532\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 1.421813\n",
      "Train Epoch: 18 [1000/60000 (2%)]\tLoss: 0.928893\n",
      "Train Epoch: 18 [2000/60000 (3%)]\tLoss: 1.013280\n",
      "Train Epoch: 18 [3000/60000 (5%)]\tLoss: 0.933985\n",
      "Train Epoch: 18 [4000/60000 (7%)]\tLoss: 1.048683\n",
      "Train Epoch: 18 [5000/60000 (8%)]\tLoss: 1.116564\n",
      "Train Epoch: 18 [6000/60000 (10%)]\tLoss: 1.308715\n",
      "Train Epoch: 18 [7000/60000 (12%)]\tLoss: 1.019637\n",
      "Train Epoch: 18 [8000/60000 (13%)]\tLoss: 1.200525\n",
      "Train Epoch: 18 [9000/60000 (15%)]\tLoss: 1.136415\n",
      "Train Epoch: 18 [10000/60000 (17%)]\tLoss: 1.049964\n",
      "Train Epoch: 18 [11000/60000 (18%)]\tLoss: 1.406315\n",
      "Train Epoch: 18 [12000/60000 (20%)]\tLoss: 1.535066\n",
      "Train Epoch: 18 [13000/60000 (22%)]\tLoss: 0.945153\n",
      "Train Epoch: 18 [14000/60000 (23%)]\tLoss: 1.126848\n",
      "Train Epoch: 18 [15000/60000 (25%)]\tLoss: 0.661194\n",
      "Train Epoch: 18 [16000/60000 (27%)]\tLoss: 1.242771\n",
      "Train Epoch: 18 [17000/60000 (28%)]\tLoss: 0.769686\n",
      "Train Epoch: 18 [18000/60000 (30%)]\tLoss: 0.960947\n",
      "Train Epoch: 18 [19000/60000 (32%)]\tLoss: 1.291945\n",
      "Train Epoch: 18 [20000/60000 (33%)]\tLoss: 1.238909\n",
      "Train Epoch: 18 [21000/60000 (35%)]\tLoss: 0.909264\n",
      "Train Epoch: 18 [22000/60000 (37%)]\tLoss: 1.541616\n",
      "Train Epoch: 18 [23000/60000 (38%)]\tLoss: 0.831123\n",
      "Train Epoch: 18 [24000/60000 (40%)]\tLoss: 0.783871\n",
      "Train Epoch: 18 [25000/60000 (42%)]\tLoss: 1.093749\n",
      "Train Epoch: 18 [26000/60000 (43%)]\tLoss: 1.318321\n",
      "Train Epoch: 18 [27000/60000 (45%)]\tLoss: 1.239196\n",
      "Train Epoch: 18 [28000/60000 (47%)]\tLoss: 0.866889\n",
      "Train Epoch: 18 [29000/60000 (48%)]\tLoss: 0.982020\n",
      "Train Epoch: 18 [30000/60000 (50%)]\tLoss: 0.847616\n",
      "Train Epoch: 18 [31000/60000 (52%)]\tLoss: 1.208315\n",
      "Train Epoch: 18 [32000/60000 (53%)]\tLoss: 1.005987\n",
      "Train Epoch: 18 [33000/60000 (55%)]\tLoss: 1.376181\n",
      "Train Epoch: 18 [34000/60000 (57%)]\tLoss: 1.004180\n",
      "Train Epoch: 18 [35000/60000 (58%)]\tLoss: 0.884617\n",
      "Train Epoch: 18 [36000/60000 (60%)]\tLoss: 1.221496\n",
      "Train Epoch: 18 [37000/60000 (62%)]\tLoss: 1.098079\n",
      "Train Epoch: 18 [38000/60000 (63%)]\tLoss: 0.945377\n",
      "Train Epoch: 18 [39000/60000 (65%)]\tLoss: 1.026397\n",
      "Train Epoch: 18 [40000/60000 (67%)]\tLoss: 0.766329\n",
      "Train Epoch: 18 [41000/60000 (68%)]\tLoss: 1.012235\n",
      "Train Epoch: 18 [42000/60000 (70%)]\tLoss: 0.976751\n",
      "Train Epoch: 18 [43000/60000 (72%)]\tLoss: 0.917766\n",
      "Train Epoch: 18 [44000/60000 (73%)]\tLoss: 1.037373\n",
      "Train Epoch: 18 [45000/60000 (75%)]\tLoss: 1.228302\n",
      "Train Epoch: 18 [46000/60000 (77%)]\tLoss: 0.842580\n",
      "Train Epoch: 18 [47000/60000 (78%)]\tLoss: 0.821516\n",
      "Train Epoch: 18 [48000/60000 (80%)]\tLoss: 1.175098\n",
      "Train Epoch: 18 [49000/60000 (82%)]\tLoss: 1.546619\n",
      "Train Epoch: 18 [50000/60000 (83%)]\tLoss: 1.188856\n",
      "Train Epoch: 18 [51000/60000 (85%)]\tLoss: 1.199910\n",
      "Train Epoch: 18 [52000/60000 (87%)]\tLoss: 1.009750\n",
      "Train Epoch: 18 [53000/60000 (88%)]\tLoss: 1.270305\n",
      "Train Epoch: 18 [54000/60000 (90%)]\tLoss: 0.872401\n",
      "Train Epoch: 18 [55000/60000 (92%)]\tLoss: 0.748653\n",
      "Train Epoch: 18 [56000/60000 (93%)]\tLoss: 1.151172\n",
      "Train Epoch: 18 [57000/60000 (95%)]\tLoss: 1.011934\n",
      "Train Epoch: 18 [58000/60000 (97%)]\tLoss: 1.236573\n",
      "Train Epoch: 18 [59000/60000 (98%)]\tLoss: 0.711345\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 1.244992\n",
      "Train Epoch: 19 [1000/60000 (2%)]\tLoss: 1.359159\n",
      "Train Epoch: 19 [2000/60000 (3%)]\tLoss: 1.140134\n",
      "Train Epoch: 19 [3000/60000 (5%)]\tLoss: 1.135183\n",
      "Train Epoch: 19 [4000/60000 (7%)]\tLoss: 0.651399\n",
      "Train Epoch: 19 [5000/60000 (8%)]\tLoss: 0.886481\n",
      "Train Epoch: 19 [6000/60000 (10%)]\tLoss: 0.906568\n",
      "Train Epoch: 19 [7000/60000 (12%)]\tLoss: 0.943656\n",
      "Train Epoch: 19 [8000/60000 (13%)]\tLoss: 1.136833\n",
      "Train Epoch: 19 [9000/60000 (15%)]\tLoss: 1.037358\n",
      "Train Epoch: 19 [10000/60000 (17%)]\tLoss: 1.189439\n",
      "Train Epoch: 19 [11000/60000 (18%)]\tLoss: 1.384130\n",
      "Train Epoch: 19 [12000/60000 (20%)]\tLoss: 1.379661\n",
      "Train Epoch: 19 [13000/60000 (22%)]\tLoss: 0.982132\n",
      "Train Epoch: 19 [14000/60000 (23%)]\tLoss: 0.966831\n",
      "Train Epoch: 19 [15000/60000 (25%)]\tLoss: 0.859740\n",
      "Train Epoch: 19 [16000/60000 (27%)]\tLoss: 1.152769\n",
      "Train Epoch: 19 [17000/60000 (28%)]\tLoss: 0.820613\n",
      "Train Epoch: 19 [18000/60000 (30%)]\tLoss: 1.063297\n",
      "Train Epoch: 19 [19000/60000 (32%)]\tLoss: 0.712228\n",
      "Train Epoch: 19 [20000/60000 (33%)]\tLoss: 1.160848\n",
      "Train Epoch: 19 [21000/60000 (35%)]\tLoss: 0.942235\n",
      "Train Epoch: 19 [22000/60000 (37%)]\tLoss: 1.482863\n",
      "Train Epoch: 19 [23000/60000 (38%)]\tLoss: 1.032220\n",
      "Train Epoch: 19 [24000/60000 (40%)]\tLoss: 1.174322\n",
      "Train Epoch: 19 [25000/60000 (42%)]\tLoss: 1.217860\n",
      "Train Epoch: 19 [26000/60000 (43%)]\tLoss: 1.436709\n",
      "Train Epoch: 19 [27000/60000 (45%)]\tLoss: 1.499834\n",
      "Train Epoch: 19 [28000/60000 (47%)]\tLoss: 0.910237\n",
      "Train Epoch: 19 [29000/60000 (48%)]\tLoss: 1.162048\n",
      "Train Epoch: 19 [30000/60000 (50%)]\tLoss: 0.793404\n",
      "Train Epoch: 19 [31000/60000 (52%)]\tLoss: 1.206870\n",
      "Train Epoch: 19 [32000/60000 (53%)]\tLoss: 1.016876\n",
      "Train Epoch: 19 [33000/60000 (55%)]\tLoss: 0.737268\n",
      "Train Epoch: 19 [34000/60000 (57%)]\tLoss: 0.821785\n",
      "Train Epoch: 19 [35000/60000 (58%)]\tLoss: 1.430907\n",
      "Train Epoch: 19 [36000/60000 (60%)]\tLoss: 1.005788\n",
      "Train Epoch: 19 [37000/60000 (62%)]\tLoss: 1.211913\n",
      "Train Epoch: 19 [38000/60000 (63%)]\tLoss: 1.045435\n",
      "Train Epoch: 19 [39000/60000 (65%)]\tLoss: 1.426425\n",
      "Train Epoch: 19 [40000/60000 (67%)]\tLoss: 0.805701\n",
      "Train Epoch: 19 [41000/60000 (68%)]\tLoss: 0.772934\n",
      "Train Epoch: 19 [42000/60000 (70%)]\tLoss: 1.001552\n",
      "Train Epoch: 19 [43000/60000 (72%)]\tLoss: 1.411860\n",
      "Train Epoch: 19 [44000/60000 (73%)]\tLoss: 0.961243\n",
      "Train Epoch: 19 [45000/60000 (75%)]\tLoss: 0.726235\n",
      "Train Epoch: 19 [46000/60000 (77%)]\tLoss: 0.855624\n",
      "Train Epoch: 19 [47000/60000 (78%)]\tLoss: 0.780795\n",
      "Train Epoch: 19 [48000/60000 (80%)]\tLoss: 1.099291\n",
      "Train Epoch: 19 [49000/60000 (82%)]\tLoss: 0.938058\n",
      "Train Epoch: 19 [50000/60000 (83%)]\tLoss: 0.646246\n",
      "Train Epoch: 19 [51000/60000 (85%)]\tLoss: 1.490371\n",
      "Train Epoch: 19 [52000/60000 (87%)]\tLoss: 1.088614\n",
      "Train Epoch: 19 [53000/60000 (88%)]\tLoss: 1.315304\n",
      "Train Epoch: 19 [54000/60000 (90%)]\tLoss: 0.891755\n",
      "Train Epoch: 19 [55000/60000 (92%)]\tLoss: 0.809733\n",
      "Train Epoch: 19 [56000/60000 (93%)]\tLoss: 0.791422\n",
      "Train Epoch: 19 [57000/60000 (95%)]\tLoss: 0.985216\n",
      "Train Epoch: 19 [58000/60000 (97%)]\tLoss: 0.867456\n",
      "Train Epoch: 19 [59000/60000 (98%)]\tLoss: 1.208259\n"
     ]
    }
   ],
   "source": [
    "model = Net_X()\n",
    "if args['cuda']:\n",
    "    model.cuda()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "for epoch in range(1, 20):\n",
    "    train(epoch)\n",
    "    #test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e7870d0d-2397-4fbf-811b-ffc4d4b07475",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = torch.Tensor(X_train[:100]).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0084a0b0-c468-4c43-a924-efc6bc1b820e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5.5119, -0.2474,  3.6570,  0.7206,  8.5064,  1.9749,  1.3811,  3.0900,\n",
       "         0.4116,  3.4834,  2.6858,  4.5241,  3.2261,  4.8166,  0.4482,  6.8343,\n",
       "         1.6463,  8.0130,  5.4927,  9.0044,  4.7836,  0.1411,  8.6223,  0.9260,\n",
       "         2.0309,  2.8689,  4.9513,  2.6598,  2.1460,  3.8980,  4.6085,  4.5731,\n",
       "         6.0273,  9.5516,  2.2443,  5.5884,  4.7176, -0.0944,  6.3089,  5.1543,\n",
       "         1.0156,  8.2999,  6.2850, 10.5866,  2.4217,  9.4644,  6.8004,  3.0856,\n",
       "         5.6328,  2.5685,  2.8682, -0.2107,  6.3912,  7.1789,  7.7293,  7.0431,\n",
       "        -0.0432,  9.7662,  3.9811,  1.2459,  3.4113,  4.4740,  5.7615,  0.2431,\n",
       "         4.1698,  4.9224,  6.5999,  2.9463,  0.9375,  1.5088,  2.0301,  7.7456,\n",
       "         1.3012,  5.6224,  2.1257,  0.6614,  2.0845,  1.1337,  0.9787,  6.0387,\n",
       "         4.6731,  1.0849,  1.5749,  5.3832,  9.9967,  7.4370,  2.4130,  8.1027,\n",
       "         0.1293,  5.2943,  7.2823,  7.1159,  3.6037,  6.0356,  8.8199, -0.2809,\n",
       "         6.7694,  8.8129,  3.3834,  1.0391], device='cuda:0',\n",
       "       grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(d.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4f9e487c-7675-47d1-bdda-56f999c4dce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2778774"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean((model(d.cuda()).cpu().detach().numpy() - y_train[:100])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5026298d-d7a5-4dda-8cab-be86cd62005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = model.to('cpu')\n",
    "\n",
    "adversary = net_adversary.double() #fc_z_agmm(n_instruments, n_hidden, dropout_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f57f02b5-36dd-49eb-b0cc-ba8128ab5feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mliv.neuralnet import ADeepCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "cd6c472a-034d-487b-b8c7-1ae64e709cb1",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n",
      "torch.Size([50, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "res = ADeepCI(learner, adversary).fit(X_1_a_j, X_2_a_j, X_1_a_k, X_2_a_k, \n",
    "                  X_1_b_j, X_2_b_j, X_1_b_k, X_2_b_k, Z, \n",
    "            learner_l2=1e-3, adversary_l2=1e-4, adversary_norm_reg=1e-3,\n",
    "            learner_lr=0.001, adversary_lr=0.001, n_epochs=3, bs=bs, train_learner_every=4, train_adversary_every=1,\n",
    "            ols_weight=0., warm_start=True, logger=None, model_dir='.', device='cuda', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "00b71fae-342e-42b8-b6bd-62c19a58007e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_final = model #torch.load(os.path.join(res.model_dir,\"epoch{}\".format(res.n_epochs - 1)))\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.scatter(X_2_a_k_t.T.squeeze().cpu().data.numpy(), model_final(X_2_a_k.cuda()).cpu().data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a41e0df7-e0bc-4bad-95dc-0a80d9bdf053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9753497711064195\n"
     ]
    }
   ],
   "source": [
    "print(np.mean((X_2_a_k_t.T.squeeze().cpu().data.numpy() - model_final(X_2_a_k.cuda()).cpu().data.numpy())**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "195ec110-5f6f-4121-9415-840af5e03a55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X_2_a_k_t.T.squeeze().cpu().data.numpy(), aa(X_2_a_k_t).cpu().data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7635456c-a698-4ab0-8785-3dabc2c6fca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.9/dist-packages (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from statsmodels) (1.23.1)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.9/dist-packages (from statsmodels) (0.5.3)\n",
      "Requirement already satisfied: pandas>=0.25 in /usr/local/lib/python3.9/dist-packages (from statsmodels) (1.4.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.9/dist-packages (from statsmodels) (21.3)\n",
      "Requirement already satisfied: scipy>=1.3 in /usr/local/lib/python3.9/dist-packages (from statsmodels) (1.8.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging>=21.3->statsmodels) (3.0.9)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25->statsmodels) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.25->statsmodels) (2.8.2)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from patsy>=0.5.2->statsmodels) (1.14.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "355ba91b-99bd-4608-a527-12000628b8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24916709463858813\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "y = model_final(X_2_a_k).cpu().data.numpy()\n",
    "x = aa(X_2_a_k_t).cpu().data.numpy()\n",
    "\n",
    "print(np.mean((y - x) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d4c6707-ef9d-49a1-a653-90c9cfd0fa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.023\n",
      "Model:                            OLS   Adj. R-squared:                  0.023\n",
      "Method:                 Least Squares   F-statistic:                     98.30\n",
      "Date:                Wed, 12 Oct 2022   Prob (F-statistic):           6.46e-23\n",
      "Time:                        04:55:32   Log-Likelihood:                 2470.4\n",
      "No. Observations:                4131   AIC:                            -4937.\n",
      "Df Residuals:                    4129   BIC:                            -4924.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1             0.0629      0.006      9.915      0.000       0.050       0.075\n",
      "const          0.9271      0.004    215.916      0.000       0.919       0.935\n",
      "==============================================================================\n",
      "Omnibus:                     3943.593   Durbin-Watson:                   2.008\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           122097.950\n",
      "Skew:                          -4.821   Prob(JB):                         0.00\n",
      "Kurtosis:                      27.827   Cond. No.                         4.23\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "x = sm.add_constant(x, prepend=False)\n",
    "\n",
    "# Fit and summarize OLS model\n",
    "mod = sm.OLS(y,x)\n",
    "\n",
    "res = mod.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a2e668-851a-4f74-bee0-fcfbf7e8c1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62562ebf-725f-45b3-96b7-4d85be76c420",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "bddb6c94227d8177e61600db041b4cc1c87a884063126e29a3bfd540ed5196fb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
