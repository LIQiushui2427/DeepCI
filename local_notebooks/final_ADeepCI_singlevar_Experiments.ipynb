{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d77d2b9-1d2b-4d84-8c0e-2b51d3a546e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-07 01:44:07.094769: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-01-07 01:44:07.319454: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-01-07 01:44:08.206320: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-07 01:44:08.206376: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-01-07 01:44:08.206380: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lqs/Desktop/vscproj/DeepCI/local_notebooks\n",
      "False\n",
      "1.13.1\n"
     ]
    }
   ],
   "source": [
    "seedNum = 888\n",
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "import random\n",
    "import statsmodels.api as sm\n",
    "tf.random.set_seed(seedNum)\n",
    "np.random.seed(seedNum)\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import sys, os\n",
    "print(os.getcwd())\n",
    "os.chdir('/home/lqs/Desktop/vscproj/DeepCI')\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### import from our files\n",
    "from mliv.dgps import get_data, get_tau_fn, fn_dict\n",
    "from mliv.neuralnet.utilities import log_metrics, plot_results, hyperparam_grid,\\\n",
    "                                     hyperparam_mult_grid, eval_performance\n",
    "from mliv.neuralnet.mnist_dgps import AbstractMNISTxz\n",
    "from mliv.neuralnet import AGMM,KernelLayerMMDGMM\n",
    "from mliv.neuralnet.rbflayer import gaussian, inverse_multiquadric\n",
    "from mliv.neuralnet import ADeepCI\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# ! pip install statsmodels\n",
    "device = torch.cuda.current_device() if torch.cuda.is_available() else None\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c249b24-2bc8-4a50-a34c-6b9b1a3e9e9d",
   "metadata": {},
   "source": [
    "# use the normal \"scale\" estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e330df4-e41d-4c1d-8c2c-02698a6d616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_epochs = 60\n",
    "# bs = 2\n",
    "\n",
    "n_epochs = 200 #80\n",
    "bs = 50\n",
    "\n",
    "J = 5\n",
    "\n",
    "rho = 0.2\n",
    "\n",
    "func_run = \"sin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8552fff8-2417-4e18-bcdb-f725e23ca923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "torch.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dc0866-6082-45f2-b228-629307f360e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function(func_str):\n",
    "    if func_str == \"abs\":\n",
    "        return (lambda x: ((np.abs(np.abs(x)-2)-1.5)/1).flatten(), \n",
    "                lambda x: ((torch.abs(torch.abs(x)-2)-1.5)/1).flatten())\n",
    "    elif func_str == \"log\":\n",
    "        return (lambda x: 2*np.log(np.abs(x)).flatten(), \n",
    "                lambda x: 2*torch.log(torch.abs(x)).flatten())\n",
    "    elif func_str == \"sin\":\n",
    "        return (lambda x: np.sin(x).flatten(), \n",
    "                lambda x: torch.sin(x).flatten())\n",
    "    else:\n",
    "        return (lambda x: np.sign(np.abs(np.abs(x)-5)-2).flatten(), \n",
    "                lambda x: torch.sign(torch.abs(torch.abs(x)-5)-2).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4ee77-a552-4026-9dac-14abf4b70b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simdata(NUM_I,seed,func,rho): #a = y_train/test, b = X_train/test\n",
    "    \n",
    "    kk = 5\n",
    "    \n",
    "    np.random.seed(seed)    \n",
    "    X_1_a_j = [] \n",
    "    X_2_a_j = [] \n",
    "    X_1_a_k = [] \n",
    "    X_2_a_k = [] \n",
    "    X_1_b_j = [] \n",
    "    X_2_b_j = [] \n",
    "    X_1_b_k = [] \n",
    "    X_2_b_k = []\n",
    "    Z       = []\n",
    "    for i in tqdm(range(0,NUM_I)):\n",
    "        #J = np.random.randint(4,10) # number of choice\n",
    "        \n",
    "        X_1_a = np.random.uniform(-1,1,J) #customer a\n",
    "        X_2_a = np.random.uniform(-kk,kk,J)  #customer a # -3 to 3 work well\n",
    "        X_1_b = np.random.uniform(-1,1,J) #customer b\n",
    "        X_2_b = np.random.uniform(-kk,kk,J) #customer b\n",
    "        \n",
    "        xi  = np.random.normal(0,0.5,J)    # same across all customers\n",
    "        \n",
    "        X_2_a = X_2_a + rho*xi  #customer a endogeneity\n",
    "        \n",
    "        X_2_b = X_2_b + rho*xi  #customer b endogeneity\n",
    "        \n",
    "        u_a   = X_1_a + 2*func(X_2_a) + xi + np.random.normal(0,3,J) # \\epsilon_{a} # originally 0.1\n",
    "        u_b   = X_1_b + 2*func(X_2_b) + xi + np.random.normal(0,3,J) # \\epsilon_{b}\n",
    "        choice_j = np.argmax(u_a) # return the index of product in the sample that customer a chose, we assume customer a as choose j\n",
    "\n",
    "        choice_k = np.argmax(u_b) # return the index of product in the sample that customer b chose, we assume customer b as choose k\n",
    "               \n",
    "        if choice_j == choice_k:\n",
    "            continue\n",
    "        else:  \n",
    "\n",
    "            X_1_a_j.append(X_1_a[choice_j])\n",
    "            X_2_a_j.append(X_2_a[choice_j])\n",
    "            X_1_a_k.append(X_1_a[choice_k])\n",
    "            X_2_a_k.append(X_2_a[choice_k])\n",
    "            \n",
    "            X_1_b_j.append(X_1_b[choice_j]) \n",
    "            X_2_b_j.append(X_2_b[choice_j]) \n",
    "            X_1_b_k.append(X_1_b[choice_k]) \n",
    "            X_2_b_k.append(X_2_b[choice_k]) \n",
    "            Z.append(np.array([X_1_a[choice_j], X_2_a[choice_j], X_1_a[choice_k], X_2_a[choice_k], \n",
    "                                   X_1_b[choice_j], X_2_b[choice_j], X_1_b[choice_k], X_2_b[choice_k]]))\n",
    "    \n",
    "    return torch.Tensor(X_1_a_j).reshape((-1,1)).double(), torch.Tensor(X_2_a_j).reshape((-1,1)).double(), \\\n",
    "torch.Tensor(X_1_a_k).reshape((-1,1)).double(), torch.Tensor(X_2_a_k).reshape((-1,1)).double(), torch.Tensor(X_1_b_j).reshape((-1,1)).double(), \\\n",
    "torch.Tensor(X_2_b_j).reshape((-1,1)).double(), torch.Tensor(X_1_b_k).reshape((-1,1)).double(), torch.Tensor(X_2_b_k).reshape((-1,1)).double(), \\\n",
    "torch.tensor(Z, dtype=torch.float64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad8027-3000-4929-b8d3-47a42661db27",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa,bb = get_function(func_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b07e4-f0c0-4b72-96d6-925250d5953a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(19104)\n",
    "X_1_a_j, X_2_a_j, X_1_a_k, X_2_a_k, X_1_b_j, X_2_b_j, X_1_b_k, X_2_b_k, Z = Simdata(10000,2,aa,rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a0f2d5-b1d7-4b6d-a85d-85b39a1c7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = (X_1_a_j[0:100]+2*torch.unsqueeze(aa(X_2_a_j[0:100]),-1))-(X_1_a_k[0:100]+2*torch.unsqueeze(aa(X_2_a_k[0:100]),-1))\n",
    "torch.sum(tmp<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82e63c-963d-4dec-9ace-4f6d403f0a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Z_agmm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Z_agmm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x  # F.log_softmax(x, dim=1)\n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "class CNN_Z_kernel(nn.Module):\n",
    "    def __init__(self, g_features=100):\n",
    "        super(CNN_Z_kernel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, g_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x  # F.log_softmax(x, dim=1)\n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "class CNN_X(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_X, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x  # F.log_softmax(x, dim=1)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e893ec27-77fd-4945-a9ee-f4ae0fb6a6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_z_kernel(n_z, n_hidden, g_features, dropout_p):\n",
    "    FC_Z_kernel = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_z, n_hidden),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_hidden, g_features),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    return FC_Z_kernel\n",
    "\n",
    "\n",
    "def fc_z_agmm(n_z, n_hidden, dropout_p):\n",
    "    FC_Z_agmm = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_z, n_hidden),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_hidden, 1),\n",
    "    )\n",
    "    return FC_Z_agmm\n",
    "\n",
    "\n",
    "def fc_x(n_t, n_hidden, dropout_p):\n",
    "    FC_X = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_t, n_hidden),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_hidden, 1),\n",
    "    )\n",
    "    return FC_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aab8430-e3fa-4560-a3da-8e0178dbdacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "n_hidden = 300\n",
    "n_instruments = 1\n",
    "dropout_p = 0.1\n",
    "\n",
    "\n",
    "class CNN_X(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_X, self).__init__()\n",
    "        self.conv1 = nn.Linear(1, k)\n",
    "        self.conv2 = nn.Linear(k, 200)\n",
    "        self.conv3 = nn.Linear(200, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        #x = F.relu(x)\n",
    "        output = 1.5*torch.nn.functional.tanh(x)\n",
    "        return output.squeeze()\n",
    "    \n",
    "\n",
    "if func_run == \"abs\":\n",
    "    net_learner1 = CNN_X()\n",
    "else:\n",
    "    net_learner1 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, k),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(k, 200),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(200, 2),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(2, 1),\n",
    "            torch.nn.Tanh(),\n",
    "            )\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "net_learner2 = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, k),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(k, 200),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(200, 1),\n",
    "            )\n",
    "\n",
    "\n",
    "learner = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(k, n_hidden), nn.LeakyReLU(),\n",
    "                       nn.Dropout(p=dropout_p), nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
    "                       nn.Dropout(p=dropout_p), nn.Linear(n_hidden, 1))\n",
    "\n",
    "adversary_fn = nn.Sequential(nn.Dropout(p=dropout_p), nn.Linear(k, n_hidden), nn.LeakyReLU(),\n",
    "                            nn.Dropout(p=dropout_p), nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
    "                            nn.Dropout(p=dropout_p), nn.Linear(n_hidden, 1))\n",
    "\n",
    "net_adversary = torch.nn.Sequential(nn.Dropout(p=0.1),\n",
    "            torch.nn.Linear(8, k),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),                        \n",
    "            torch.nn.Linear(k, 200), #200\n",
    "            torch.nn.LeakyReLU(),\n",
    "            nn.Dropout(p=0.1),                        \n",
    "            torch.nn.Linear(200, 1),\n",
    "            )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026298d-d7a5-4dda-8cab-be86cd62005a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner1 = net_learner1.double()\n",
    "\n",
    "learner2 = net_learner2.double()\n",
    "\n",
    "adversary = net_adversary.double() #fc_z_agmm(n_instruments, n_hidden, dropout_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f481d19-7de8-4794-b01d-aac7f308be0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res1 = ADeepCI(learner1, adversary).fit(X_1_a_j, X_2_a_j, X_1_a_k, X_2_a_k, \n",
    "                  X_1_b_j, X_2_b_j, X_1_b_k, X_2_b_k, Z, \n",
    "                  learner_l2=1e-3, adversary_l2=1e-4, adversary_norm_reg=1e-3,\n",
    "                  learner_lr=0.001, adversary_lr=0.001, n_epochs=n_epochs, bs=bs, train_learner_every=1, train_adversary_every=1,\n",
    "                  ols_weight=0., warm_start=False, logger=None, model_dir='.', device=device, verbose=1)\n",
    "\n",
    "model_final1 = torch.load(os.path.join(res1.model_dir,\"epoch{}\".format(res1.n_epochs-1)))\n",
    "\n",
    "res2 = ADeepCI(learner2, adversary).fit(X_1_a_j, X_2_a_j, X_1_a_k, X_2_a_k, \n",
    "            X_1_b_j, X_2_b_j, X_1_b_k, X_2_b_k, Z, \n",
    "            learner_l2=1e-3, adversary_l2=1e-4, adversary_norm_reg=1e-3,\n",
    "            learner_lr=0.001, adversary_lr=0.001, n_epochs=n_epochs, bs=bs, train_learner_every=1, train_adversary_every=1,\n",
    "            ols_weight=0., warm_start=False, logger=None, model_dir='.', device=device, verbose=1)\n",
    "            \n",
    "model_final2 = torch.load(os.path.join(res2.model_dir,\"epoch{}\".format(res2.n_epochs - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b9098-991b-40be-a13a-7b2dc83f9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_2_a_k.T.squeeze().cpu().data.numpy(), aa(X_2_a_k).cpu().data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871c54ad-2b82-4d0d-9f34-d958c4346925",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(19104+1)\n",
    "X_1_a_j_test, X_2_a_j_test, X_1_a_k_test, X_2_a_k_test, X_1_b_j_test, X_2_b_j_test, X_1_b_k_test, X_2_b_k_test, Z_test = Simdata(10000,2,aa,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fffc74-f5fc-4dde-a45f-7bc7b44e0fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MSEs = []\n",
    "Biases = []\n",
    "\n",
    "for kk in range (0,3):\n",
    "    np.random.seed(kk)\n",
    "    X_1_a_j, X_2_a_j, X_1_a_k, X_2_a_k, X_1_b_j, X_2_b_j, X_1_b_k, X_2_b_k, Z = Simdata(10000,2,aa,rho)\n",
    "    \n",
    "    res1 = ADeepCI(learner1, adversary).fit(X_1_a_j, X_2_a_j, X_1_a_k, X_2_a_k, \n",
    "                  X_1_b_j, X_2_b_j, X_1_b_k, X_2_b_k, Z, \n",
    "            learner_l2=1e-3, adversary_l2=1e-4, adversary_norm_reg=1e-3,\n",
    "            learner_lr=0.0001, adversary_lr=0.0001, n_epochs=n_epochs, bs=bs, train_learner_every=1, train_adversary_every=1,\n",
    "            ols_weight=0., warm_start=False, logger=None, model_dir='.', device=device, verbose=1)\n",
    "    model_final = torch.load(os.path.join(res1.model_dir,\"epoch{}\".format(res1.n_epochs - 1)))\n",
    "    \n",
    "    y = model_final(X_2_a_k).cpu().data.numpy()\n",
    "    if y.ndim == 2:\n",
    "        x = aa(X_2_a_k).cpu().data.numpy()[:, np.newaxis]\n",
    "    else:\n",
    "        x = aa(X_2_a_k).cpu().data.numpy()\n",
    "\n",
    "\n",
    "    MSE = np.mean((y - x) ** 2)\n",
    "    MSEs.append(MSE)\n",
    "    Bias = np.mean(y-x)\n",
    "    Biases.append(Bias)\n",
    "y_pred = 2*(X_1_a_j_test.T.squeeze().cpu().data.numpy() + model_final(X_2_a_j_test).squeeze().cpu().data.numpy() > \\\n",
    "           X_1_a_k_test.T.squeeze().cpu().data.numpy() + model_final(X_2_a_k_test).squeeze().cpu().data.numpy())-1\n",
    "y = 1*((y_pred+100)>0) # all ones\n",
    "f1_score(y,y_pred)    \n",
    "print('MSEs',np.mean(MSEs))\n",
    "print('Biases',np.mean(Biases))\n",
    "print(\"Y shape == X shape: {}\".format(y.shape == x.shape))\n",
    "\n",
    "#Fit and summarize OLS model\n",
    "x = sm.add_constant(x, prepend=False)\n",
    "mod = sm.OLS(y,x)\n",
    "res = mod.fit()\n",
    "print(res.summary())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.scatter(X_2_a_k.T.squeeze().cpu().data.numpy(), model_final(X_2_a_k).cpu().data.numpy())\n",
    "plt.savefig(func_run+str(J)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca49db5-7af3-469e-914f-8551393fb8b5",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76beb017-db9b-4f3e-8fc3-1487df6628ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84f693-406a-47d0-b3c8-63d5de1aff2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.concatenate((X_1_a_j.cpu().data.numpy(),X_2_a_j.cpu().data.numpy()),axis = 1)\n",
    "X_2 = np.concatenate((X_1_a_k.cpu().data.numpy(),X_2_a_k.cpu().data.numpy()),axis = 1)\n",
    "y_1 = np.ones(X_1.shape[0])\n",
    "y_2 = np.zeros(X_1.shape[0])\n",
    "\n",
    "X = np.concatenate((X_1,X_2), axis = 0)\n",
    "y = np.concatenate((y_1,y_2), axis = 0)\n",
    "\n",
    "clf = LogisticRegression(random_state=0).fit(X, y)\n",
    "\n",
    "X_a_j_test = np.concatenate((X_1_a_j_test.cpu().data.numpy(),X_2_a_j_test.cpu().data.numpy()),axis = 1)\n",
    "\n",
    "y_pred_clf = 2*clf.predict(X_a_j_test)-1 #1 or -1\n",
    "y_clf = 1*((y_pred_clf+100)>0) # all ones\n",
    "\n",
    "f1_score(y_clf,y_pred_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1d8ee-f8bf-462d-b3e0-42b5f31fdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from sklearn.metrics import accuracy_score, precision_recall_curve, precision_score, recall_score, roc_auc_score\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "method = func_run\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def compute_stats(d1_train, d2_train, y_train, d1_test, d2_test, y_test, N, mode):\n",
    "\n",
    "\n",
    "    print(\"J:\",J, \"rho:\",rho, \"method:\",method, \"mode:\",mode)\n",
    "\n",
    "\n",
    "    torch.manual_seed(2)    # reproducible\n",
    "\n",
    "    x1 = torch.tensor(d1_train, dtype=torch.float32)\n",
    "    x2 = torch.tensor(d2_train, dtype=torch.float32)\n",
    "    y  = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "    # torch can only train on Variable, so convert them to Variable\n",
    "    x1, x2, y = Variable(x1), Variable(x2), Variable(y)\n",
    "\n",
    "\n",
    "    if mode == 'PPHI':\n",
    "        net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, 1),\n",
    "            torch.nn.Flatten(0, 1)\n",
    "            )\n",
    "    elif mode == 'polyPPHI':\n",
    "        net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, 1),\n",
    "            torch.nn.Flatten(0, 1)\n",
    "            )\n",
    "    else:\n",
    "        net = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, 300),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(300, 200),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(200, 1),\n",
    "            )\n",
    "\n",
    "\n",
    "    # print(net)  # net architecture\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.1)\n",
    "\n",
    "    if mode == 'PPHI':\n",
    "        p = torch.tensor([1])\n",
    "        x10 = torch.unsqueeze(x1[:,0],dim=1)\n",
    "        x20 = torch.unsqueeze(x2[:,0],dim=1)\n",
    "        x11 = torch.unsqueeze(x1[:,1:],dim=1).pow(p)\n",
    "        x21 = torch.unsqueeze(x2[:,1:],dim=1).pow(p)\n",
    "    elif mode == 'polyPPHI':\n",
    "        p = torch.tensor([1,2])\n",
    "        x10 = torch.unsqueeze(x1[:,0],dim=1)\n",
    "        x20 = torch.unsqueeze(x2[:,0],dim=1)\n",
    "        x11 = torch.unsqueeze(x1[:,1:],dim=1).pow(p)\n",
    "        x21 = torch.unsqueeze(x2[:,1:],dim=1).pow(p)\n",
    "    else:\n",
    "        p = torch.tensor([1])\n",
    "        x10 = torch.unsqueeze(x1[:,0],dim=1)\n",
    "        x20 = torch.unsqueeze(x2[:,0],dim=1)\n",
    "        x11 = torch.unsqueeze(x1[:,1:],dim=1)\n",
    "        x21 = torch.unsqueeze(x2[:,1:],dim=1)\n",
    "\n",
    "\n",
    "    def deepci_loss(first, second, y):\n",
    "        diff1 = torch.reshape(first - second, (-1,))\n",
    "        diff = diff1*(y>0) + (-diff1)*(y<0)\n",
    "        loss = torch.mean(torch.minimum(torch.zeros(diff.size()),diff)**2)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCH = 100\n",
    "    torch_dataset = Data.TensorDataset(x10, x11, x20, x21, y)\n",
    "\n",
    "    loader = Data.DataLoader(\n",
    "        dataset=torch_dataset, \n",
    "        batch_size=BATCH_SIZE, \n",
    "        shuffle=True, num_workers=2,)\n",
    "    iteration = 0\n",
    "\n",
    "    # start training\n",
    "    for epoch in range(EPOCH):\n",
    "        for step, (batch_x10, batch_x11, batch_x20, batch_x21, batch_y) in enumerate(loader): # for each training step\n",
    "\n",
    "            b_x10 = Variable(batch_x10)\n",
    "            b_x11 = Variable(batch_x11)\n",
    "            b_x20 = Variable(batch_x20)\n",
    "            b_x21 = Variable(batch_x21)\n",
    "            b_y = Variable(batch_y)\n",
    "\n",
    "            prediction_1 = b_x10 + net(b_x11)     # input x and predict based on x\n",
    "            prediction_2 = b_x20 + net(b_x21)\n",
    "\n",
    "            loss = deepci_loss(prediction_1, prediction_2, b_y)     # must be (1. nn output, 2. target)\n",
    "            #if iteration%500 == 0:\n",
    "            #    print(loss)\n",
    "            iteration +=1    \n",
    "            optimizer.zero_grad()   # clear gradients for next train\n",
    "            loss.backward()         # backpropagation, compute gradients\n",
    "            optimizer.step()        # apply gradients\n",
    "\n",
    "\n",
    "    x1_test = torch.tensor(d1_test, dtype=torch.float32)\n",
    "    x2_test = torch.tensor(d2_test, dtype=torch.float32)\n",
    "    y       = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    if mode == 'PPHI':\n",
    "        x10_test = torch.unsqueeze(x1_test[:,0],dim=1)\n",
    "        x20_test = torch.unsqueeze(x2_test[:,0],dim=1)\n",
    "        x11_test = torch.unsqueeze(x1_test[:,1:],dim=1).pow(p)\n",
    "        x21_test = torch.unsqueeze(x2_test[:,1:],dim=1).pow(p)\n",
    "    elif mode == 'polyPPHI':\n",
    "        x10_test = torch.unsqueeze(x1_test[:,0],dim=1)\n",
    "        x20_test = torch.unsqueeze(x2_test[:,0],dim=1)\n",
    "        x11_test = torch.unsqueeze(x1_test[:,1:],dim=1).pow(p)\n",
    "        x21_test = torch.unsqueeze(x2_test[:,1:],dim=1).pow(p)\n",
    "    else:\n",
    "        x10_test = torch.unsqueeze(x1_test[:,0],dim=1)\n",
    "        x20_test = torch.unsqueeze(x2_test[:,0],dim=1)\n",
    "        x11_test = torch.unsqueeze(x1_test[:,1:],dim=1)\n",
    "        x21_test = torch.unsqueeze(x2_test[:,1:],dim=1)\n",
    "\n",
    "\n",
    "    if mode == 'NN':\n",
    "        y_pred = 2*(x10_test.flatten() + net(x11_test).data.numpy().flatten() > \\\n",
    "                x20_test.flatten() + net(x21_test).data.numpy().flatten())-1\n",
    "    else:\n",
    "        y_pred = 2*((x10_test + net(x11_test)).flatten() > \\\n",
    "                (x20_test + net(x21_test)).flatten())-1\n",
    "\n",
    "    f1 = f1_score(y.numpy(),y_pred.numpy())\n",
    "    \n",
    "    yy = net(x11_test).cpu().data.numpy()\n",
    "    xx = aa(x1_test[:,1:]).cpu().data.numpy()[:, np.newaxis]\n",
    "\n",
    "    xx = sm.add_constant(xx, prepend=False)\n",
    "    # Fit and summarize OLS model\n",
    "    mod = sm.OLS(yy,xx)\n",
    "    res = mod.fit()\n",
    "    print(res.summary())\n",
    "    print(yy.shape)\n",
    "    print(xx.shape)\n",
    "    \n",
    "    MSE = np.mean((yy - xx) ** 2)\n",
    "    RMSE = np.sqrt(MSE)\n",
    "    MAE = np.mean(np.abs(yy - xx))\n",
    "    CE = np.mean(np.abs(yy - xx) / np.abs(xx))\n",
    "    accuracy_score = np.mean((yy - xx) == 0)\n",
    "    Bias = np.mean(yy-xx)\n",
    "        \n",
    "    return f1, RMSE, MAE, CE, accuracy_score, Bias\n",
    "\n",
    "\n",
    "def simulateData(N, J, K, func, rho, sim):\n",
    "    np.random.seed(sim)\n",
    "    D = np.random.uniform(-1,1,(N,J + 1,K))\n",
    "    x1 = np.array([])\n",
    "    x2 = np.array([])\n",
    "    y  = np.array([])\n",
    "    for n in range(N):\n",
    "        err = np.random.normal(0,0.5) # epsilon_m\n",
    "        D[n,1:,1] = 5*D[n,1:,1] + rho*err\n",
    "        D[n,1:,0] = D[n,1:,0] \n",
    "        choice = np.argmax(D[n,:,0] + 2*func(D[n,:,1:]) + np.random.normal(0,3,(1,J + 1)) + err) # x_0+f(x_1) for each product\n",
    "        if choice == 0:\n",
    "            continue\n",
    "        for j in range(1, J+1):\n",
    "            if j == choice:\n",
    "                continue\n",
    "            else:\n",
    "                #print(D[n,choice,:])\n",
    "                if x1.size == 0:\n",
    "                    x1 = D[n,choice,:]\n",
    "                    x2 = D[n,j,:]\n",
    "                    y  = np.array([1])\n",
    "                else:    \n",
    "                    x1 = np.c_[x1, D[n,choice,:]]\n",
    "                    x2 = np.c_[x2, D[n,j,:]]\n",
    "                    y  = np.c_[y,np.array([1])]\n",
    "    return x1.T, x2.T, y.T\n",
    "\n",
    "\n",
    "print(method)\n",
    "\n",
    "\n",
    "def runConfig(J, rho, method, mode):\n",
    "    N = 2000\n",
    "    K = 2\n",
    "    a, b = get_function(method)\n",
    "    MSEs = []\n",
    "    f1 = []\n",
    "    RMSE = []\n",
    "    MAE = []\n",
    "    CE = []\n",
    "    accuracy_score = []\n",
    "    for i in range(1,2): \n",
    "        d1_train, d2_train, y_train = simulateData(N, J, K, a, rho, i)\n",
    "        d1_test,  d2_test,  y_test  = simulateData(N, J, K, a, rho, i+1)\n",
    "        f1, RMSE, MAE, CE, accuracy_score, Bias = compute_stats(d1_train, d2_train, y_train, d1_test, d2_test, y_test, method, mode)\n",
    "        MSEs.append(MSE)\n",
    "        Biases.append(Bias)\n",
    "\n",
    "    return np.mean(MSEs), np.mean(Biases), f1, RMSE, MAE, CE, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e46101-ac6b-423f-bf40-20b9d81f6f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function(func_str):\n",
    "    if func_str == \"abs\":\n",
    "        return (lambda x: (-1+0.4*np.abs(x@np.ones((1,1)))).flatten(), \n",
    "                lambda x: (-1+0.4*torch.abs(x@np.ones((1,1)))).flatten())\n",
    "    elif func_str == \"log\":\n",
    "        return (lambda x: 2*np.log(np.abs(x@np.ones((1,1)))).flatten(), \n",
    "                lambda x: 2*torch.log(torch.abs(x@np.ones((1,1)))).flatten())\n",
    "    elif func_str == \"sin\":\n",
    "        return (lambda x: np.sin(x@np.ones((1,1))).flatten(), \n",
    "                lambda x: torch.sin(x@np.ones((1,1))).flatten())\n",
    "    else:\n",
    "        return (lambda x: np.sign(np.abs(np.abs(x@np.ones((1,1)))-5)-2).flatten(), \n",
    "                lambda x: torch.sign(torch.abs(torch.abs(x@np.ones((1,1)))-5)-2).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1260198d-bb5e-4dc2-b88f-b0624bafedc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'polyPPHI'\n",
    "\n",
    "MSE, Bias, f1, RMSE, MAE, CE, accuracy_score = runConfig(5, 0.5, method, mode)\n",
    "print('MSE: ', MSE, 'Bias: ', Bias, 'f1: ', f1, 'RMSE: ', RMSE, 'MAE: ', MAE, 'CE: ', CE, 'accuracy_score: ', accuracy_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3975f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'PPHI'\n",
    "\n",
    "MSE, Bias, f1, RMSE, MAE, CE, accuracy_score = runConfig(5, 0.5, method, mode)\n",
    "print('MSE: ', MSE, 'Bias: ', Bias, 'f1: ', f1, 'RMSE: ', RMSE, 'MAE: ', MAE, 'CE: ', CE, 'accuracy_score: ', accuracy_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "34752fb930ccda7383b9ea105c0ef5936eec2182d44ee5b0b4b02e4beea6e55f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
