{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Magic functions\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext tensorboard\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "#import \n",
    "\n",
    "seedNum = 888\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seedNum)\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "np.random.seed(seedNum)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from datetime import datetime\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "#import autokeras as ak\n",
    "#import keras_tuner as kt\n",
    "\n",
    "\n",
    "#load data\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "(example_X_train,example_y_train) = (X_train[:2000], y_train[:2000])\n",
    "(example_X_test,example_y_test) = (X_test[:2000], y_test[:2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = np.random.choice(np.arange(start=0.0, stop=10.0, step=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = y_train\n",
    "b = X_train\n",
    "\n",
    "sample = np.array(random.sample(list(np.arange(a.shape[0])),6+1)) # a list of index\n",
    "sample = np.expand_dims(sample, axis=1)\n",
    "ej = np.concatenate([a[i] for i in sample],axis = None) # a list of number on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_function(func_str):\n",
    "    if func_str == \"abs\":\n",
    "        return (lambda x: np.abs(x@np.ones((1,1))).flatten(), \n",
    "                lambda x: torch.abs(x@np.ones((1,1))).flatten())\n",
    "    elif func_str == \"log\":\n",
    "        return (lambda x: np.log(np.abs(x@np.ones((1,1)))).flatten(), \n",
    "                lambda x: torch.log(torch.abs(x@np.ones((1,1)))).flatten())\n",
    "    elif func_str == \"sin\":\n",
    "        return (lambda x: np.sin(x@np.ones((1,1))).flatten(), \n",
    "                lambda x: torch.sin(x@np.ones((1,1))).flatten())\n",
    "    else:\n",
    "        return (lambda x: np.sign(np.abs(np.abs(x)-5)-2)@np.ones((1,1)).flatten(), \n",
    "                lambda x: torch.sign(torch.abs(torch.abs(x)-5)-2)@np.ones((1,1)).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Simdata(NUM_I,seed,func,rho): #a = y_train/test, b = X_train/test\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    b = np.expand_dims(np.random.uniform(-10,10,60000), axis=1) # b is the observable input\n",
    "    a = func(b) # a is the answer\n",
    "    \n",
    "    pa = [] #product a, which is product chosen by customer a, but unchosen by paired customer b\n",
    "    pb = [] #product b, which is product unchosen by customer a, but chosen by paired customer b\n",
    "    ea = []\n",
    "    eb = []\n",
    "    \n",
    "    #price_a = []\n",
    "    #price_b = []\n",
    "    \n",
    "    ca_type = []  #customer a type\n",
    "    cb_type = []  #customer b type\n",
    "    \n",
    "    g_ca_pa = [] #g funtion in utility function, which -z_a*price_product_a\n",
    "    g_ca_pb = [] #g funtion in utility function, which -z_a*price_product_b\n",
    "    g_cb_pa = []\n",
    "    g_cb_pb = []\n",
    "    \n",
    "    for i in tqdm(range(0,NUM_I)):\n",
    "        J = np.random.randint(2,10) # number of choice\n",
    "        sample = np.array(random.sample(list(np.arange(a.shape[0])),J+1)) # a list of index\n",
    "        sample = np.expand_dims(sample, axis=1)\n",
    "\n",
    "        za = np.random.choice(np.arange(start=-10.1, stop=10.1, step=0.1)) # customer type\n",
    "        zb = -np.random.choice(np.arange(start=-10.1, stop=10.1, step=0.1)) # customer type\n",
    "        # print('z0')\n",
    "        # print(za)\n",
    "        # print('z1')\n",
    "        # print(zb)\n",
    "        ej = np.concatenate([a[i] for i in sample],axis = None) # a list of number on images\n",
    "        ej = [i for i in ej.tolist()]\n",
    "        ej = np.float_(ej)\n",
    "        ej = np.expand_dims(ej,axis = 1)\n",
    "        pj = [torch.Tensor(b[i]) for i in sample]\n",
    "        #pj = tf.squeeze(pj,axis = 1)\n",
    "\n",
    "        #print(pj)\n",
    "        # print('ej')\n",
    "        # print(ej)\n",
    "        # print('=======')\n",
    "\n",
    "        gamma = np.random.normal(0,0.1,(J+1,1)) \n",
    "        # print('gamma')\n",
    "        # print(gamma)\n",
    "        # print('=======')\n",
    "        ksai = rho*ej+gamma\n",
    "        # print('rho*ej')\n",
    "        # print(rho*ej)\n",
    "        # print('=======')         \n",
    "        # print('ksai')\n",
    "        # print(ksai)\n",
    "        # print('=======')        \n",
    "        price = np.random.uniform(0,5,(J+1,1)) #remove 0.8x\n",
    "\n",
    "        # print('price')\n",
    "        # print(price)\n",
    "        # print('=======')\n",
    "        ind = np.ones((J+ 1,1)) # edited by JZ\n",
    "        \n",
    "        epsilon_a = np.random.normal(0,.01,(J+1,1))# not specifiy\n",
    "        epsilon_b = np.random.normal(0,.01,(J+1,1))# not specifiy\n",
    "        \n",
    "        # print('epsilon')\n",
    "        # print(epsilon_a)\n",
    "        # print('=======')\n",
    "        #print(-np.log(ej+1))\n",
    "        utility_a = -za*price + -ej_func(ej)*za + ksai + epsilon_a ## JZ edit: remove +1\n",
    "        #utility_a = -za*price + -np.exp(ej+1)*za + ksai + epsilon_a\n",
    "\n",
    "\n",
    "        # print('utility_a')\n",
    "        # print(utility_a)\n",
    "        # print('=======')\n",
    "        utility_b = -zb*price + -ej_func(ej)*zb + ksai + epsilon_b ## JZ edit: remove +1\n",
    "        #utility_b = -zb*price + -np.exp(ej+1)*zb + ksai + epsilon_b\n",
    "        #-np.log(ej)\n",
    "        # print('utility_b')\n",
    "        # print(utility_b)\n",
    "        # print('=======')\n",
    "        choice_a = np.argmax(utility_a) # return the index of product in the sample that customer a chose\n",
    "\n",
    "        choice_b = np.argmax(utility_b) # return the index of product in the sample that customer b chose\n",
    "        # print('choice_a')\n",
    "        # print(choice_a)\n",
    "        # print('=======')\n",
    "        # print('choice_b')\n",
    "        # print(choice_b)\n",
    "        # print('=======')\n",
    "        \n",
    "        #if choice_a or choice_b == 0:\n",
    "            #continue\n",
    "\n",
    "        if choice_a == choice_b:\n",
    "            continue\n",
    "        else:  \n",
    "\n",
    "            pa.append(pj[choice_a])\n",
    "            pb.append(pj[choice_b])\n",
    "            ea.append(ej[choice_a])\n",
    "            eb.append(ej[choice_b])\n",
    "\n",
    "\n",
    "\n",
    "            ca_type.append(ind[0]*za)  \n",
    "            cb_type.append(ind[0]*zb) \n",
    "\n",
    "            g_ca_pa.append(price[choice_a]*za)\n",
    "            g_ca_pb.append(price[choice_b]*za)\n",
    "            g_cb_pa.append(price[choice_a]*zb)\n",
    "            g_cb_pb.append(price[choice_b]*zb)\n",
    "                \n",
    "                \n",
    "                         \n",
    "    #pa = torch.cat(pa, out=torch.Tensor(len(pa), 28,28))\n",
    "    #pb = torch.cat(pb, out=torch.Tensor(len(pb), 28,28))\n",
    "    \n",
    "    pa = torch.unsqueeze(torch.Tensor(pa),dim = -1)\n",
    "    pb = torch.unsqueeze(torch.Tensor(pb),dim = -1)\n",
    "\n",
    "    \n",
    "    ea = torch.Tensor(ea)\n",
    "    eb = torch.Tensor(eb)\n",
    "    \n",
    "   \n",
    "    ca_type = torch.Tensor(ca_type)\n",
    "    cb_type = torch.Tensor(cb_type)\n",
    "    g_ca_pa = torch.Tensor(g_ca_pa)\n",
    "    g_ca_pb = torch.Tensor(g_ca_pb)\n",
    "    g_cb_pa = torch.Tensor(g_cb_pa)\n",
    "    g_cb_pb = torch.Tensor(g_cb_pb)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "      \n",
    "    \n",
    "    return pa, pb, ea, eb, ca_type, cb_type, g_ca_pa, g_ca_pb, g_cb_pa, g_cb_pb  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ej_func(ej):\n",
    "    return ej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa,bb = get_function(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:48<00:00, 206.68it/s]\n"
     ]
    }
   ],
   "source": [
    "pa, pb, ea, eb, ca_type, cb_type, g_ca_pa, g_ca_pb, g_cb_pa, g_cb_pb = Simdata(10000,2,aa,0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving the objects:\n",
    "with open('sim_deepci_singlevar.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([pa, pb,ea,eb, ca_type,cb_type,g_ca_pa,g_ca_pb,g_cb_pa,g_cb_pb], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/notebooks/AdversarialGMM/local_notebooks', '/usr/lib/python39.zip', '/usr/lib/python3.9', '/usr/lib/python3.9/lib-dynload', '', '/usr/local/lib/python3.9/dist-packages', '/usr/lib/python3/dist-packages', '/notebooks/AdversarialGMM/', '/notebooks/AdversarialGMM/']\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path\n",
    "print(sys.path)\n",
    "sys.path.append('/notebooks/AdversarialGMM/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('sim_deepci_singlevar.pkl', 'rb') as f:\n",
    "    pa, pb, ea, eb, ca_type, cb_type, g_ca_pa, g_ca_pb, g_cb_pa, g_cb_pb = pickle.load(f)\n",
    "    \n",
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "### module imports\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "### import from our files\n",
    "from mliv.dgps import get_data, get_tau_fn, fn_dict\n",
    "from mliv.neuralnet.utilities import log_metrics, plot_results, hyperparam_grid,\\\n",
    "                                     hyperparam_mult_grid, eval_performance\n",
    "from mliv.neuralnet.mnist_dgps import AbstractMNISTxz\n",
    "from mliv.neuralnet import AGMM,KernelLayerMMDGMM\n",
    "from mliv.neuralnet.rbflayer import gaussian, inverse_multiquadric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.cuda.current_device() if torch.cuda.is_available() else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_Z_agmm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_Z_agmm, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x  # F.log_softmax(x, dim=1)\n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "class CNN_Z_kernel(nn.Module):\n",
    "    def __init__(self, g_features=100):\n",
    "        super(CNN_Z_kernel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, g_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x  # F.log_softmax(x, dim=1)\n",
    "        return output.squeeze()\n",
    "\n",
    "\n",
    "class CNN_X(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_X, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, 28, 28)\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = x  # F.log_softmax(x, dim=1)\n",
    "        return output.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fc_z_kernel(n_z, n_hidden, g_features, dropout_p):\n",
    "    FC_Z_kernel = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_z, n_hidden),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_hidden, g_features),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "    return FC_Z_kernel\n",
    "\n",
    "\n",
    "def fc_z_agmm(n_z, n_hidden, dropout_p):\n",
    "    FC_Z_agmm = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_z, n_hidden),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_hidden, 1),\n",
    "    )\n",
    "    return FC_Z_agmm\n",
    "\n",
    "\n",
    "def fc_x(n_t, n_hidden, dropout_p):\n",
    "    FC_X = nn.Sequential(\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_t, n_hidden),\n",
    "        nn.LeakyReLU(),\n",
    "        nn.Dropout(p=dropout_p),\n",
    "        nn.Linear(n_hidden, 1),\n",
    "    )\n",
    "    return FC_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1_a_j = -g_ca_pa\n",
    "X_1_a_k = -g_ca_pb\n",
    "X_1_b_k = -g_cb_pb\n",
    "X_1_b_j = -g_cb_pa\n",
    "\n",
    "X_2_a_j = pa\n",
    "X_2_a_k = pb\n",
    "X_2_b_k = pb\n",
    "X_2_b_j = pa\n",
    "\n",
    "z_a = ca_type\n",
    "z_b = cb_type\n",
    "\n",
    "Z = g_ca_pa - g_ca_pb + g_cb_pb - g_cb_pa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 500\n",
    "n_hidden = 100\n",
    "net_learner = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, k),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(k, 200),\n",
    "            torch.nn.LeakyReLU(),\n",
    "            torch.nn.Linear(200, 1),\n",
    "            )\n",
    "\n",
    "#learner = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_t, n_hidden), nn.LeakyReLU(),\n",
    "#                        nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
    "#                        nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
    "\n",
    "#adversary_fn = nn.Sequential(nn.Dropout(p=p), nn.Linear(n_z, n_hidden), nn.LeakyReLU(),\n",
    "#                             nn.Dropout(p=p), nn.Linear(n_hidden, n_hidden), nn.ReLU(),\n",
    "#                             nn.Dropout(p=p), nn.Linear(n_hidden, 1))\n",
    "\n",
    "net_adversary = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, 1),\n",
    "            torch.nn.Flatten(0, 1)\n",
    "            )\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = net_learner\n",
    "\n",
    "n_instruments = 1\n",
    "n_hidden = 100\n",
    "dropout_p = 0.1\n",
    "adversary = net_adversary #fc_z_agmm(n_instruments, n_hidden, dropout_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mliv.neuralnet import ADeepCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ADeepCI(learner, adversary).fit(X_1_a_j, X_2_a_j, X_1_a_k, X_2_a_k, \n",
    "                  X_1_b_j, X_2_b_j, X_1_b_k, X_2_b_k, Z, z_a, z_b,\n",
    "            learner_l2=1e-3, adversary_l2=1e-4, adversary_norm_reg=1e-3,\n",
    "            learner_lr=0.001, adversary_lr=0.001, n_epochs=100, bs=100, train_learner_every=1, train_adversary_every=1,\n",
    "            ols_weight=0., warm_start=False, logger=None, model_dir='.', device=None, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_final = torch.load(os.path.join(res.model_dir,\"epoch{}\".format(res.n_epochs - 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.scatter(model_final(pb).cpu().data.numpy(),eb.T.squeeze().cpu().data.numpy())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.6003134 , 2.0740824 , 0.09482794, ..., 2.2298036 , 0.55139816,\n",
       "       2.16771   ], dtype=float32)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.abs(pb.cpu().data.numpy())).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST_Experiments.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
