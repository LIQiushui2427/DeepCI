{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP\n",
    "\n",
    "We generate $n$ samples from the data generating process:\n",
    "\n",
    "\\begin{align}\n",
    "z \\sim & N(\\mu=0, \\sigma=I_d)\\\\\n",
    "v \\sim & N(\\mu=0, \\sigma=I_d)\\\\\n",
    "x = & \\gamma z + v\\\\\n",
    "y \\sim & N(\\mu=\\langle x + 4 \\nu, \\theta\\rangle, \\sigma=1)\n",
    "\\end{align}\n",
    "\n",
    "where $z$ is a $d$-dimensional instrument, $x$ is a $d$-dimensional treatment, $v$ is an unobserved confounder. Each instrument $z_i$ is an instrument for treatment $x_i$. The coefficient $\\theta$ is $s$-sparse. The parameter $\\gamma$ controls the strength of the instrument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, ElasticNetCV, ElasticNet, LassoCV\n",
    "from mliv.linear import TSLasso, OptimisticHedgeVsOptimisticHedge, StochasticOptimisticHedgeVsOptimisticHedge,\\\n",
    "                        ProxGradientVsHedge, SubGradientVsHedge, L2OptimisticHedgeVsOGD, L2ProxGradient,\\\n",
    "                        L2SubGradient\n",
    "from mliv.linear.utilities import cross_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "n = 600\n",
    "d = 1000\n",
    "s = 2\n",
    "gamma = 1\n",
    "true_coefs = np.zeros(d)\n",
    "true_coefs[:s] = (2*np.random.binomial(1, .5, size=s)-1) #np.random.uniform(-1, 1, size=s)\n",
    "z = np.random.normal(0, 1, size=(n, d))\n",
    "v = np.random.normal(0, 1, size=(n, d))\n",
    "Gamma = gamma * np.eye(d)\n",
    "x = z @ Gamma + v\n",
    "#x[:, :s] = gamma * z[:, :s] - v[:, :s]\n",
    "y = np.dot(x + v, true_coefs)#+ np.random.normal(0, 1, size=(n,))\n",
    "print(true_coefs[:s])\n",
    "plt.hist(x[:, s])\n",
    "plt.hist(x[:, 1])\n",
    "plt.show()\n",
    "z_test = np.random.normal(0, 1, size=(n, d))\n",
    "v_test = np.random.normal(0, 1, size=(n, 1))\n",
    "x_test = z_test @ Gamma + v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direct = LassoCV(cv=3).fit(x, y)\n",
    "direct_coefs = direct.coef_\n",
    "print(\"Estimated non-zero coefs: \", direct_coefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(direct_coefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, direct_coefs - true_coefs)**2)))\n",
    "plt.plot(direct_coefs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.min(np.linalg.eigvalsh(Gamma.T @ Gamma))\n",
    "gamma/(8*s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two Stage Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tslasso = TSLasso(first_stage=Lasso(alpha=.01)).fit(z, x, y)\n",
    "tslasso_coefs = tslasso.coef_\n",
    "print(\"Estimated non-zero coefs: \", tslasso_coefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(tslasso_coefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, tslasso_coefs - true_coefs)**2)))\n",
    "plt.plot(tslasso_coefs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = OptimisticHedgeVsOptimisticHedge(B=3, lambda_theta=gamma/(8*s),\n",
    "                                       eta_theta=.5,\n",
    "                                       eta_w=.5,\n",
    "                                       n_iter=10000, tol=.0001, sparsity=None).fit(z, x, y)\n",
    "coefs = est.coef\n",
    "print(\"Maximum violation: \", est.max_violation_)\n",
    "print(\"Estimated non-zero coefs: \", coefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(coefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, coefs - true_coefs)**2)))\n",
    "print(\"Min/Max: \", est.min_response_loss_, est.max_response_loss_)\n",
    "print(\"Dualit Gap: \", est.duality_gap_)\n",
    "print(\"Iters: \", est.n_iters_)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(est.coef_)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(est.w_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stochest = StochasticOptimisticHedgeVsOptimisticHedge(B=3, lambda_theta=gamma/(8*s),\n",
    "                                       eta_theta=.1, eta_w=.1,\n",
    "                                       n_iter=1000, tol=1/n).fit(z, x, y)\n",
    "stochcoefs = stochest.coef\n",
    "print(\"Maximum violation: \", stochest.max_violation_)\n",
    "print(\"Estimated non-zero coefs: \", stochcoefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(stochcoefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, stochcoefs - true_coefs)**2)))\n",
    "print(\"Min/Max: \", stochest.min_response_loss_, stochest.max_response_loss_)\n",
    "print(\"Dualit Gap: \", stochest.duality_gap_)\n",
    "print(\"Iters: \", stochest.n_iters_)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(stochest.coef_)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(stochest.w_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(true_coefs, direct_coefs, label='Lasso')\n",
    "plt.scatter(true_coefs, tslasso_coefs, label='2SLasso')\n",
    "plt.scatter(true_coefs, coefs, label='SparseIV')\n",
    "plt.scatter(true_coefs, stochcoefs, label='StochasticSparseIV')\n",
    "plt.plot(np.linspace(np.min(true_coefs), np.max(true_coefs), 10),\n",
    "         np.linspace(np.min(true_coefs), np.max(true_coefs), 10), '--', label='x=y')\n",
    "plt.xlabel('true coefficient')\n",
    "plt.ylabel('estimated coefficient')\n",
    "plt.legend()\n",
    "plt.savefig('true_v_est_sparse_linear.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stochest.w_)\n",
    "plt.xlabel('variable')\n",
    "plt.ylabel('dual parameter')\n",
    "plt.savefig('duals.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ell2 error:\", np.linalg.norm(coefs-true_coefs, ord=np.inf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimistic Proximal Gradient vs Optimistic MWU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = ProxGradientVsHedge(B=3, lambda_theta=gamma/(8*s),\n",
    "                          eta_theta=1, eta_w=1,\n",
    "                          n_iter=10000, tol=1/n).fit(z, x, y)\n",
    "coefs = est.coef\n",
    "print(\"Maximum violation: \", est.max_violation_)\n",
    "print(\"Estimated non-zero coefs: \", coefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(coefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, coefs - true_coefs)**2)))\n",
    "print(\"Min/Max: \", est.min_response_loss_, est.max_response_loss_)\n",
    "print(\"Dualit Gap: \", est.duality_gap_)\n",
    "print(\"Iters: \", est.n_iters_)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(coefs)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(est.w_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simultaneous Descent: Subgradient Descent vs MWU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = SubGradientVsHedge(B=3, lambda_theta=gamma/(8*s),\n",
    "                         eta_theta='auto', eta_w='auto',\n",
    "                         n_iter=10000, tol=1/n**(2/3)).fit(z, x, y)\n",
    "coefs = est.coef\n",
    "print(\"Maximum violation: \", est.max_violation_)\n",
    "print(\"Estimated non-zero coefs: \", coefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(coefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, coefs - true_coefs)**2)))\n",
    "print(\"Min/Max: \", est.min_response_loss_, est.max_response_loss_)\n",
    "print(\"Dualit Gap: \", est.duality_gap_)\n",
    "print(\"Iters: \", est.n_iters_)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(coefs)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(est.w_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2 Adversary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(1456)\n",
    "n = 5000\n",
    "d_x = 10\n",
    "d_z = 100\n",
    "s = 2\n",
    "gamma = 1\n",
    "true_coefs = np.zeros(d_x)\n",
    "true_coefs[1] = 2*(2*np.random.binomial(1, .5,)-1)\n",
    "sigma_z = 1\n",
    "z = np.random.normal(0, sigma_z, size=(n, d_z))\n",
    "v = np.random.normal(0, 1, size=(n, 1))\n",
    "Gamma = np.random.normal(0, 1, size=(d_z, d_x))\n",
    "Gamma[d_z//2:, 0] = 0\n",
    "Gamma[:d_z//2, 1] = 0\n",
    "Gamma /= np.linalg.norm(Gamma, ord=2, axis=0, keepdims=True)\n",
    "x = z @ Gamma + v\n",
    "y = np.dot(x, true_coefs) + v[:, 0] #+ np.random.normal(0, 1, size=(n,))\n",
    "print(true_coefs[:s])\n",
    "\n",
    "z_test = np.random.normal(0, 1, size=(n, d_z))\n",
    "v_test = np.random.normal(0, 1, size=(n, 1))\n",
    "x_test = z_test @ Gamma + v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = np.min(np.linalg.eigvalsh(Gamma.T @ Gamma)) * (sigma_z**2)\n",
    "gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = TSLasso(first_stage=ElasticNet(l1_ratio=.05, alpha=0.001)).fit(z, x, y)\n",
    "coefs = est.coef_\n",
    "print(\"Estimated non-zero coefs: \", coefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(coefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, coefs - true_coefs)**2)))\n",
    "plt.plot(coefs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = L2OptimisticHedgeVsOGD(B=3, tol=1/n, lambda_theta=gamma/(8*s),\n",
    "                     n_iter=10000, eta_theta=.1, eta_w=.1, sparsity=None).fit(z, x, y)\n",
    "coefs = est.coef\n",
    "print(\"Maximum violation: \", est.max_violation_)\n",
    "print(\"Estimated non-zero coefs: \", coefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(coefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, coefs - true_coefs)**2)))\n",
    "print(\"Min/Max: \", est.min_response_loss_, est.max_response_loss_)\n",
    "print(\"Dualit Gap: \", est.duality_gap_)\n",
    "print(\"Iterations: \", est.n_iters_)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(coefs)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(est.w_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = L2ProxGradient(B=2, tol=.0001, lambda_theta=.0001,\n",
    "                     n_iter=10000).fit(z, x, y)\n",
    "coefs = est.coef\n",
    "print(\"Maximum violation: \", est.max_violation_)\n",
    "print(\"Estimated non-zero coefs: \", coefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(coefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, coefs - true_coefs)**2)))\n",
    "print(\"Min/Max: \", est.min_response_loss_, est.max_response_loss_)\n",
    "print(\"Dualit Gap: \", est.duality_gap_)\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(coefs)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(est.w_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est = L2SubGradient(B=2, lambda_theta=gamma/(8*s), n_iter=1000).fit(z, x, y)\n",
    "coefs = est.coef\n",
    "print(\"Maximum violation: \", est.max_violation_)\n",
    "print(\"Estimated non-zero coefs: \", coefs[:s])\n",
    "print(\"ell2 error:\", np.linalg.norm(coefs-true_coefs, ord=2))\n",
    "print(\"RMSE:\", np.sqrt(np.mean(np.dot(x_test, coefs - true_coefs)**2)))\n",
    "print(\"Min/Max: \", est.min_response_loss_, est.max_response_loss_)\n",
    "print(\"Dualit Gap: \", est.duality_gap_)\n",
    "plt.plot(coefs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
